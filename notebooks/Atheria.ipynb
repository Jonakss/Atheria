{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56ab4c1b-eb89-49ac-b423-9e220fbe8abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.8.0+cu128)\n",
      "Requirement already satisfied: numpy in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (3.8.2)\n",
      "Requirement already satisfied: imageio in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2.37.0)\n",
      "Requirement already satisfied: pillow in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (12.0.0)\n",
      "Requirement already satisfied: opencv-python in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (4.11.0.86)\n",
      "Requirement already satisfied: tifffile in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (2025.10.16)\n",
      "Requirement already satisfied: ffmpeg in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (1.4)\n",
      "Requirement already satisfied: filelock in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (4.60.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (3.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/zeus/miniconda3/envs/cloudspace/lib/python3.12/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install torch numpy matplotlib imageio pillow opencv-python tifffile ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45eb125d-6172-4a34-a60e-9d4fdb2a8056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Detected 1 GPU.\n",
      "Training Checkpoint Directory: checkpoints_optimized\n",
      "Large Simulation Checkpoint Directory: large_sim_checkpoints_1024\n",
      "\n",
      "‚ÑπÔ∏è Not configured to use a specific input model. Will look for locally trained models in CHECKPOINT_DIR.\n",
      "Input model configuration (optional) completed.\n",
      "Global parameters set.\n",
      "QCA_State, QCA_Operator_Deep, and Aetheria_Motor classes defined.\n",
      "Visualization and state checkpointing functions defined.\n",
      "QC_Trainer_v3 class defined.\n",
      "--- STARTING AETHERIA PIPELINE EXECUTION ---\n",
      "\n",
      ">>> TRAINING PHASE (PHASE 5) SKIPPED <<<\n",
      "Initializing training-size motor to load model...\n",
      "üì¶ Detected latest .pth file (assumed trained model): checkpoints_optimized/PEF_Deep_v3_G256_Eps2_1762202475_FINAL.pth. Attempting to load weights...\n",
      "‚úÖ Model weights loaded successfully into Aetheria_Motor_Train.\n",
      "\n",
      "============================================================\n",
      ">>> STARTING POST-TRAINING VISUALIZATION PHASE (PHASE 6) <<<\n",
      "============================================================\n",
      "Generating 1500 frames with the M-Law on 256x256 grid...\n",
      "-> Capturing frame 150/1500...\n",
      "-> Capturing frame 300/1500...\n",
      "-> Capturing frame 450/1500...\n",
      "-> Capturing frame 600/1500...\n",
      "-> Capturing frame 750/1500...\n",
      "-> Capturing frame 900/1500...\n",
      "-> Capturing frame 1050/1500...\n",
      "-> Capturing frame 1200/1500...\n",
      "-> Capturing frame 1350/1500...\n",
      "-> Capturing frame 1500/1500...\n",
      "‚úÖ Visualization frame capture completed.\n",
      "\n",
      "--- SAVING VISUALIZATION VIDEOS (Training Size) ---\n",
      "‚ùå Error saving visualization videos (training size): TiffWriter.write() got an unexpected keyword argument 'fps'\n",
      "\n",
      ">>> POST-TRAINING VISUALIZATION PHASE (PHASE 6) COMPLETED <<<\n",
      "\n",
      ">>> PROLONGED LARGE SIMULATION PHASE (PHASE 7) SKIPPED <<<\n",
      "\n",
      "--- AETHERIA PIPELINE EXECUTION FINISHED ---\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# ==============================================================================\n",
    "# AETHERIA: MASTER PYTHON SCRIPT (Consolidated from Notebook)\n",
    "# ==============================================================================\n",
    "# This script consolidates all code cells from the AETHERIA notebook\n",
    "# into a single runnable Python file.\n",
    "#\n",
    "# To run:\n",
    "# 1. Ensure all required libraries are installed:\n",
    "#    pip install torch numpy matplotlib imageio pillow\n",
    "# 2. Configure the parameters in the \"PHASE 3: GLOBAL PARAMETERS\" section below.\n",
    "# 3. Run the script from your terminal:\n",
    "#    python your_script_name.py\n",
    "# ==============================================================================\n",
    "\n",
    "# %% [markdown]\n",
    "# # PHASE 0: SETUP & IMPORTS\n",
    "# ---\n",
    "# Import necessary libraries.\n",
    "# Note: IPython imports are removed as this is a standalone script.\n",
    "# %% [code]\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import imageio.v2 as imageio\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "import re # To parse checkpoint filenames\n",
    "import gc # Import garbage collection for memory management\n",
    "# from IPython.display import display # Removed (Notebook-specific)\n",
    "# from IPython.display import Video # Removed (Notebook-specific)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 0.1: SETUP AND CONTROL CONSTANTS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Set up the device to use CUDA if available, otherwise use CPU.\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "if torch.cuda.is_available():\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Detected {torch.cuda.device_count()} GPUs. DataParallel will be managed internally.\")\n",
    "    else:\n",
    "        print(\"Detected 1 GPU.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU detected. Training and simulation will be slow on CPU.\")\n",
    "\n",
    "# Enable cuDNN auto-tuner for potential speed improvements on GPU.\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# --- Checkpoint Directory Configuration ---\n",
    "# Directory to save training checkpoints.\n",
    "CHECKPOINT_DIR = \"checkpoints_optimized\"\n",
    "# Directory to save large simulation checkpoints.\n",
    "LARGE_SIM_CHECKPOINT_DIR = \"large_sim_checkpoints_1024\" # Directory for simulation\n",
    "\n",
    "# Create directories if they don't exist.\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "os.makedirs(LARGE_SIM_CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"Training Checkpoint Directory: {CHECKPOINT_DIR}\")\n",
    "print(f\"Large Simulation Checkpoint Directory: {LARGE_SIM_CHECKPOINT_DIR}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 0.3: INPUT MODEL CONFIGURATION (OPTIONAL)\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Set to True if you want to attempt loading a trained model from a specific input path.\n",
    "# If set to False, the script will look for locally trained models in CHECKPOINT_DIR.\n",
    "USE_INPUT_MODEL = False\n",
    "\n",
    "# If USE_INPUT_MODEL is True, specify the path to the .pth file of the model you want to load.\n",
    "INPUT_MODEL_PATH = \"\" # <--- Specify the path here if USE_INPUT_MODEL is True\n",
    "\n",
    "# Internal variable to track if an input model will be used.\n",
    "USING_INPUT_MODEL_FLAG = False\n",
    "\n",
    "if USE_INPUT_MODEL and INPUT_MODEL_PATH and os.path.exists(INPUT_MODEL_PATH):\n",
    "    print(f\"\\n‚úÖ Configured to use input model from: {INPUT_MODEL_PATH}\")\n",
    "    USING_INPUT_MODEL_FLAG = True\n",
    "elif USE_INPUT_MODEL and (not INPUT_MODEL_PATH or not os.path.exists(INPUT_MODEL_PATH)):\n",
    "    print(f\"\\n‚ö†Ô∏è USE_INPUT_MODEL is True, but INPUT_MODEL_PATH is not specified or the file does not exist at '{INPUT_MODEL_PATH}'.\")\n",
    "    print(\"Will look for locally trained models in CHECKPOINT_DIR instead.\")\n",
    "    USING_INPUT_MODEL_FLAG = False\n",
    "else:\n",
    "    print(\"\\n‚ÑπÔ∏è Not configured to use a specific input model. Will look for locally trained models in CHECKPOINT_DIR.\")\n",
    "    USING_INPUT_MODEL_FLAG = False\n",
    "\n",
    "USE_INPUT_MODEL_FOR_TRAINING_RESUME = False\n",
    "if USE_INPUT_MODEL_FOR_TRAINING_RESUME and USING_INPUT_MODEL_FLAG:\n",
    "     print(\"‚ö†Ô∏è Warning: USE_INPUT_MODEL_FOR_TRAINING_RESUME is True and an input model was configured.\")\n",
    "     print(\"This may cause issues if the input model is not a full training checkpoint.\")\n",
    "     print(\"Ensure the file at INPUT_MODEL_PATH is a full training checkpoint if you continue.\")\n",
    "\n",
    "print(\"Input model configuration (optional) completed.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # PHASE 3: GLOBAL PARAMETERS & CONFIGURATION\n",
    "# ---\n",
    "# Configure all tunable parameters for the pipeline here.\n",
    "# %% [code]\n",
    "# ==============================================================================\n",
    "# --- PHASE 3: GLOBAL PARAMETERS & CONFIGURATION ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- Execution Control ---\n",
    "# Define which parts of the pipeline will run.\n",
    "RUN_TRAINING = False          # Set to True to run the training phase.\n",
    "RUN_POST_TRAINING_VIZ = True # Set to True to run post-training visualization (at training size).\n",
    "RUN_LARGE_SIM = False          # Set to True to run the prolonged large simulation.\n",
    "\n",
    "CONTINUE_TRAINING = False      # Set to True to resume training from the last checkpoint in CHECKPOINT_DIR.\n",
    "\n",
    "# --- Simulation Parameters (Training Size) ---\n",
    "GRID_SIZE_TRAINING = 256      # Grid size for training (optimized)\n",
    "D_STATE = 32                  # Dimension of the quantum state (optimized)\n",
    "HIDDEN_CHANNELS = 256         # Channels in the M-Law's deep network (optimized)\n",
    "\n",
    "# --- Optimized Training Parameters ---\n",
    "EPISODES_TO_ADD = 1000         # Number of training episodes to run\n",
    "STEPS_PER_EPISODE = 150        # Number of simulation steps per episode\n",
    "LR_RATE_M = 5e-6              # Learning rate for the optimizer\n",
    "PERSISTENCE_COUNT = 10        # (k in BPTT-k)\n",
    "\n",
    "# --- Reward Parameters (Optimized Annealing) ---\n",
    "ALPHA_START = 3.0             # Initial weight for R_Density_Target\n",
    "ALPHA_END = 30.0              # Final weight for R_Density_Target\n",
    "GAMMA_START = 3.0             # Initial weight for R_Stability\n",
    "GAMMA_END = 0.6               # Final weight for R_Stability\n",
    "BETA_CAUSALITY = 3.0          # Fixed weight for R_Causality\n",
    "\n",
    "# --- Weights for New Rewards ---\n",
    "LAMBDA_ACTIVITY_VAR = 1.0     # Weight for R_Activity_Var\n",
    "LAMBDA_VELOCIDAD = 0.5        # Weight for R_Velocidad\n",
    "\n",
    "# --- \"Target-Seeking\" and Penalty Parameters (Optimized) ---\n",
    "TARGET_STD_DENSITY = 1.2      # Target standard deviation for density\n",
    "EXPLOSION_THRESHOLD = 0.7     # Max density per cell to trigger penalty\n",
    "EXPLOSION_PENALTY_MULTIPLIER = 20.0 # Multiplier for explosion penalty\n",
    "\n",
    "# --- Stagnation Parameters (Optimized) ---\n",
    "STAGNATION_WINDOW = 500       # Episodes without improvement before stagnation\n",
    "MIN_LOSS_IMPROVEMENT = 5e-6   # Minimum required loss improvement\n",
    "\n",
    "# --- Reactivation Parameters (Optimized) ---\n",
    "REACTIVATION_COUNT = 2        # Number of times to attempt reactivation\n",
    "REACTIVATION_STATE_MODE = 'random' # 'random', 'seeded', 'complex_noise'\n",
    "REACTIVATION_LR_MULTIPLIER = 0.5 # Factor to multiply LR by on reactivation\n",
    "\n",
    "# --- Gradient Clipping ---\n",
    "GRADIENT_CLIP = 0.85          # Threshold for gradient clipping\n",
    "\n",
    "# --- Checkpointing Frequency (Training) ---\n",
    "SAVE_EVERY_EPISODES = 50      # Save a training checkpoint every N episodes.\n",
    "\n",
    "# --- Post-Training Visualization Parameters (Training Size) ---\n",
    "NUM_FRAMES_VIZ = 1500         # Number of simulation steps for the viz video\n",
    "FPS_VIZ_TRAINING = 24         # Frames per second for the viz video\n",
    "\n",
    "# --- Large Simulation Parameters (Inference Size) ---\n",
    "GRID_SIZE_INFERENCE = 1250     # Grid size for large simulation\n",
    "NUM_INFERENCE_STEPS = 50000    # Total number of large simulation steps\n",
    "\n",
    "# --- Initialization Configuration (Inference) ---\n",
    "INITIAL_STATE_MODE_INFERENCE = 'random' # 'random', 'seeded', 'complex_noise'\n",
    "LOAD_STATE_CHECKPOINT_INFERENCE = True # Load existing large sim state checkpoint\n",
    "STATE_CHECKPOINT_PATH_INFERENCE = \"\" # Specific path to state checkpoint\n",
    "\n",
    "# --- Checkpointing Frequency (Large Simulation) ---\n",
    "LARGE_SIM_CHECKPOINT_INTERVAL = 1500 # Save a large sim state checkpoint every N steps.\n",
    "\n",
    "# --- Video Saving Parameters (Large Simulation) ---\n",
    "VIDEO_FPS = 20                # Frames per second for generated videos\n",
    "VIDEO_SAVE_INTERVAL_STEPS = 50 # Save a video frame every N simulation steps\n",
    "VIDEO_DOWNSCALE_FACTOR = 1    # Factor to reduce video resolution (1 = no downscale)\n",
    "VIDEO_QUALITY = 8             # Video quality (0-51, lower is better)\n",
    "\n",
    "# --- Real-time Visualization Parameters (Large Simulation) ---\n",
    "REAL_TIME_VIZ_INTERVAL = 100  # Show a frame every N steps (set to None or 0 to disable)\n",
    "REAL_TIME_VIZ_TYPE = 'phase' # 'density', 'channels', 'magnitude', 'phase', 'change'\n",
    "REAL_TIME_VIZ_DOWNSCALE = 4   # Factor to reduce resolution for real-time display\n",
    "\n",
    "print(\"Global parameters set.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # PHASE 1: QCA ENGINE CLASSES\n",
    "# ---\n",
    "# Definition of `QCA_State`, `QCA_Operator_Deep`, and `Aetheria_Motor`.\n",
    "# %% [code]\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1.1: QCA_State Class (The State of the Universe)\n",
    "# ------------------------------------------------------------------------------\n",
    "class QCA_State:\n",
    "    def __init__(self, size, d_state):\n",
    "        self.size = size\n",
    "        self.d_state = d_state\n",
    "        self.x_real = torch.zeros(1, size, size, d_state, device=DEVICE)\n",
    "        self.x_imag = torch.zeros(1, size, size, d_state, device=DEVICE)\n",
    "\n",
    "    def _reset_state_random(self):\n",
    "        \"\"\"Initializes the state with low-amplitude noise and normalizes it.\"\"\"\n",
    "        noise_r = (torch.rand(1, self.size, self.size, self.d_state, device=DEVICE) * 2 - 1) * 1e-2\n",
    "        noise_i = (torch.rand(1, self.size, self.size, self.d_state, device=DEVICE) * 2 - 1) * 1e-2\n",
    "        self.x_real.data = noise_r\n",
    "        self.x_imag.data = noise_i\n",
    "        self.normalize_state()\n",
    "\n",
    "    def _reset_state_seeded(self):\n",
    "        \"\"\"Initializes the state with a vacuum and a 'seed' of activity in the center.\"\"\"\n",
    "        self.x_real.data.fill_(0)\n",
    "        self.x_imag.data.fill_(0)\n",
    "        center_x, center_y = self.size // 2, self.size // 2\n",
    "        seed_size = max(1, self.size // 64)\n",
    "        for dx in range(-seed_size, seed_size + 1):\n",
    "            for dy in range(-seed_size, seed_size + 1):\n",
    "                if 0 <= center_x + dx < self.size and 0 <= center_y + dy < self.size:\n",
    "                        if self.d_state > 3: # Ensure enough channels exist\n",
    "                            self.x_real[0, center_y + dy, center_x + dx, 0] = 0.5\n",
    "                            self.x_imag[0, center_y + dy, center_x + dx, 1] = 0.5\n",
    "                            self.x_real[0, center_y + dy, center_x + dx, 2] = -0.5\n",
    "                            self.x_imag[0, center_y + dy, center_x + dx, 3] = -0.5\n",
    "        self.normalize_state()\n",
    "\n",
    "    def _reset_state_complex_noise(self):\n",
    "        \"\"\"Initializes the state with a structured complex noise pattern.\"\"\"\n",
    "        y_coords, x_coords = torch.meshgrid(torch.linspace(-1, 1, self.size, device=DEVICE),\n",
    "                                            torch.linspace(-1, 1, self.size, device=DEVICE),\n",
    "                                            indexing='ij')\n",
    "        radial_dist = torch.sqrt(x_coords**2 + y_coords**2)\n",
    "        angle = torch.atan2(y_coords, x_coords)\n",
    "        pattern1_r = torch.sin(x_coords * 10 + angle * 5) * 0.1\n",
    "        pattern1_i = torch.cos(y_coords * 12 + angle * 6) * 0.1\n",
    "        pattern2_r = torch.sin(radial_dist * 15 + x_coords * 8) * 0.05\n",
    "        pattern2_i = torch.cos(radial_dist * 18 + y_coords * 9) * 0.05\n",
    "        noise_r = (torch.rand(self.size, self.size, self.d_state, device=DEVICE) * 2 - 1) * 1e-3\n",
    "        noise_i = (torch.rand(self.size, self.size, self.d_state, device=DEVICE) * 2 - 1) * 1e-3\n",
    "        if self.d_state > 0: noise_r[:, :, 0] += pattern1_r\n",
    "        if self.d_state > 1: noise_i[:, :, 1] += pattern1_i\n",
    "        if self.d_state > 2: noise_r[:, :, 2] += pattern2_r\n",
    "        if self.d_state > 3: noise_i[:, :, 3] += pattern2_i\n",
    "        self.x_real.data = noise_r.unsqueeze(0).to(DEVICE)\n",
    "        self.x_imag.data = noise_i.unsqueeze(0).to(DEVICE)\n",
    "        self.normalize_state()\n",
    "\n",
    "    def normalize_state(self):\n",
    "        \"\"\"Normalizes the state vector in each cell to conserve probability.\"\"\"\n",
    "        prob_sq = self.x_real.pow(2) + self.x_imag.pow(2)\n",
    "        norm = torch.sqrt(prob_sq.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "        self.x_real.data = self.x_real.data / norm\n",
    "        self.x_imag.data = self.x_imag.data / norm\n",
    "\n",
    "    def get_cat(self):\n",
    "        \"\"\"Concatenates real/imag tensors into (B, C, H, W) format for CNNs.\"\"\"\n",
    "        x_real_c = self.x_real.permute(0, 3, 1, 2)\n",
    "        x_imag_c = self.x_imag.permute(0, 3, 1, 2)\n",
    "        return torch.cat([x_real_c, x_imag_c], dim=1)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1.2: QCA_Operator_Deep Class (The \"Deep\" M-Law)\n",
    "# ------------------------------------------------------------------------------\n",
    "class QCA_Operator_Deep(nn.Module):\n",
    "    def __init__(self, d_state, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.d_state = d_state\n",
    "        # 3x3 Neighborhood convolution (non-trainable, center-masked)\n",
    "        self.conv_neighbors = nn.Conv2d(2*d_state, 2*d_state*8, kernel_size=3,\n",
    "                                        padding=1, groups=2*d_state, bias=False)\n",
    "        weights = torch.ones(2*d_state*8, 1, 3, 3)\n",
    "        weights[:, 0, 1, 1] = 0.0 # Zero out the center weight.\n",
    "        self.conv_neighbors.weight.data = weights\n",
    "        self.conv_neighbors.weight.requires_grad = False\n",
    "\n",
    "        # Trainable 1x1 Convolutional MLP\n",
    "        self.processing_net = nn.Sequential(\n",
    "            nn.Conv2d(2 * d_state * 8, hidden_channels, kernel_size=1, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=1, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=1, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, hidden_channels, kernel_size=1, bias=False),\n",
    "            nn.ELU(inplace=True),\n",
    "            nn.Conv2d(hidden_channels, 8 * d_state, kernel_size=1, bias=False)\n",
    "        )\n",
    "\n",
    "        # Trainable bias parameters\n",
    "        self.M_bias_real = nn.Parameter(torch.zeros(d_state))\n",
    "        self.M_bias_imag = nn.Parameter(torch.zeros(d_state))\n",
    "\n",
    "    def forward(self, x_cat):\n",
    "        \"\"\"Applies the evolution operator.\"\"\"\n",
    "        x_neighbors = self.conv_neighbors(x_cat.to(self.conv_neighbors.weight.device))\n",
    "        F_int = self.processing_net(x_neighbors)\n",
    "\n",
    "        F_int = F_int.squeeze(0).permute(1, 2, 0) # (H, W, Channels)\n",
    "        H, W, C = F_int.shape\n",
    "        D4 = 4 * self.d_state\n",
    "\n",
    "        F_int_real_raw = F_int[:, :, :D4]\n",
    "        F_int_imag_raw = F_int[:, :, D4:]\n",
    "\n",
    "        F_int_real = F_int_real_raw.reshape(H, W, 4, self.d_state).mean(dim=2) * 0.1\n",
    "        F_int_imag = F_int_imag_raw.reshape(H, W, 4, self.d_state).mean(dim=2) * 0.1\n",
    "\n",
    "        return F_int_real, F_int_imag\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 1.3: Aetheria_Motor Class (The Evolution Engine)\n",
    "# ------------------------------------------------------------------------------\n",
    "class Aetheria_Motor:\n",
    "    def __init__(self, size, d_state, operator_model):\n",
    "        self.size = size\n",
    "        self.d_state = d_state\n",
    "        # Send the operator model to the device\n",
    "        self.operator = operator_model.to(DEVICE)\n",
    "        if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
    "            print(f\"Using {torch.cuda.device_count()} GPUs for the operator.\")\n",
    "            self.operator = nn.DataParallel(self.operator)\n",
    "\n",
    "        self.state = QCA_State(size, d_state)\n",
    "\n",
    "    def evolve_step(self):\n",
    "        \"\"\"Evolves the QCA state one time step.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            prev_state = self.state\n",
    "            x_cat = prev_state.get_cat()\n",
    "\n",
    "            if isinstance(self.operator, nn.DataParallel):\n",
    "                x_cat = x_cat.to(self.operator.device_ids[0])\n",
    "            else:\n",
    "                 x_cat = x_cat.to(DEVICE)\n",
    "\n",
    "            delta_real, delta_imag = self.operator(x_cat)\n",
    "\n",
    "            if isinstance(self.operator, nn.DataParallel):\n",
    "                bias_real = self.operator.module.M_bias_real\n",
    "                bias_imag = self.operator.module.M_bias_imag\n",
    "            else:\n",
    "                bias_real = self.operator.M_bias_real\n",
    "                bias_imag = self.operator.M_bias_imag\n",
    "\n",
    "            new_real = prev_state.x_real.squeeze(0) + delta_real + bias_real.to(delta_real.device)\n",
    "            new_imag = prev_state.x_imag.squeeze(0) + delta_imag + bias_imag.to(delta_imag.device)\n",
    "\n",
    "            prob_sq = new_real.pow(2) + new_imag.pow(2)\n",
    "            norm = torch.sqrt(prob_sq.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "            next_real = new_real / norm\n",
    "            next_imag = new_imag / norm\n",
    "\n",
    "            self.state.x_real.data = next_real.unsqueeze(0)\n",
    "            self.state.x_imag.data = next_imag.unsqueeze(0)\n",
    "\n",
    "print(\"QCA_State, QCA_Operator_Deep, and Aetheria_Motor classes defined.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # PHASE 4: VISUALIZATION & CHECKPOINTING FUNCTIONS\n",
    "# ---\n",
    "# Helper functions for generating visualization frames and saving/loading state.\n",
    "# %% [code]\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4.1: Visualization Helper Functions\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def downscale_frame(frame, downscale_factor):\n",
    "    \"\"\"Downscales an image frame (numpy array) using PIL.\"\"\"\n",
    "    if downscale_factor <= 1:\n",
    "        return frame\n",
    "    height, width = frame.shape[:2]\n",
    "    new_height, new_width = height // downscale_factor, width // downscale_factor\n",
    "    img = Image.fromarray(frame)\n",
    "    img_resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    return np.array(img_resized)\n",
    "\n",
    "\n",
    "def get_density_frame_gpu(state):\n",
    "    \"\"\"Generates a frame visualizing total probability density.\"\"\"\n",
    "    prob_sq = state.x_real.pow(2) + state.x_imag.pow(2)\n",
    "    density_map = prob_sq.squeeze(0).sum(dim=2).detach()\n",
    "    d_min, d_max = density_map.min(), density_map.max()\n",
    "    norm_factor = d_max - d_min\n",
    "    if norm_factor < 1e-8:\n",
    "        normalized_density = torch.zeros_like(density_map).to(state.x_real.device)\n",
    "    else:\n",
    "        normalized_density = (density_map - d_min) / norm_factor\n",
    "    normalized_density_clamped = normalized_density.clamp(0.0, 1.0)\n",
    "    R = normalized_density_clamped\n",
    "    G = torch.zeros_like(normalized_density_clamped)\n",
    "    B = 1.0 - normalized_density_clamped\n",
    "    img_rgb = torch.stack([R, G, B], dim=2).clamp(0.0, 1.0)\n",
    "    final_image = (img_rgb * 255).byte().cpu().numpy()\n",
    "    return final_image\n",
    "\n",
    "def get_channel_frame_gpu(state, num_channels=3):\n",
    "    \"\"\"Generates a frame visualizing the first N channels as RGB.\"\"\"\n",
    "    prob_sq = state.x_real.pow(2) + state.x_imag.pow(2)\n",
    "    combined_image = torch.zeros(state.size, state.size, 3, device=state.x_real.device)\n",
    "\n",
    "    num_channels_to_viz = min(num_channels, state.d_state)\n",
    "    if num_channels_to_viz == 0:\n",
    "        return (combined_image * 255).byte().cpu().numpy()\n",
    "\n",
    "    for i in range(num_channels_to_viz):\n",
    "        channel_data = prob_sq[0, :, :, i].detach()\n",
    "        ch_min, ch_max = channel_data.min(), channel_data.max()\n",
    "        if (ch_max - ch_min) < 1e-8:\n",
    "            channel_scaled = torch.zeros_like(channel_data)\n",
    "        else:\n",
    "            channel_scaled = (channel_data - ch_min) / (ch_max - ch_min)\n",
    "        color_index = i % 3\n",
    "        combined_image[:, :, color_index] += channel_scaled * (1.0 / num_channels_to_viz)\n",
    "\n",
    "    final_image = (combined_image.clamp(0, 1) * 255).byte().cpu().numpy()\n",
    "    return final_image\n",
    "\n",
    "def get_state_magnitude_frame_gpu(state):\n",
    "    \"\"\"Generates a frame visualizing the state vector magnitude (grayscale).\"\"\"\n",
    "    prob_sq = state.x_real.pow(2) + state.x_imag.pow(2)\n",
    "    magnitude_map = torch.sqrt(prob_sq.squeeze(0).sum(dim=2) + 1e-8).detach()\n",
    "    m_min, m_max = magnitude_map.min(), magnitude_map.max()\n",
    "    norm_factor = m_max - m_min\n",
    "    if norm_factor < 1e-8:\n",
    "        normalized_magnitude = torch.zeros_like(magnitude_map).to(state.x_real.device)\n",
    "    else:\n",
    "        normalized_magnitude = (magnitude_map - m_min) / norm_factor\n",
    "    normalized_magnitude_clamped = normalized_magnitude.clamp(0.0, 1.0)\n",
    "    img_gray = normalized_magnitude_clamped\n",
    "    img_rgb = torch.stack([img_gray, img_gray, img_gray], dim=2)\n",
    "    final_image = (img_rgb * 255).byte().cpu().numpy()\n",
    "    return final_image\n",
    "\n",
    "def get_state_phase_frame_gpu(state):\n",
    "    \"\"\"Generates a frame visualizing the state vector phase (mapped to Hue).\"\"\"\n",
    "    sum_real = state.x_real.squeeze(0).sum(dim=2)\n",
    "    sum_imag = state.x_imag.squeeze(0).sum(dim=2)\n",
    "    phase_map = torch.atan2(sum_imag, sum_real).detach()\n",
    "    normalized_phase = (phase_map + torch.pi) / (2 * torch.pi)\n",
    "    normalized_phase_clamped = normalized_phase.clamp(0.0, 1.0)\n",
    "    hue = normalized_phase_clamped\n",
    "    R = torch.sin(2 * torch.pi * hue + torch.pi/2) * 0.5 + 0.5\n",
    "    G = torch.sin(2 * torch.pi * hue + torch.pi*3/2) * 0.5 + 0.5\n",
    "    B = torch.sin(2 * torch.pi * hue + torch.pi*5/2) * 0.5 + 0.5\n",
    "    img_rgb = torch.stack([R, G, B], dim=2).clamp(0.0, 1.0)\n",
    "    final_image = (img_rgb * 255).byte().cpu().numpy()\n",
    "    return final_image\n",
    "\n",
    "def get_state_change_magnitude_frame_gpu(state, prev_state):\n",
    "    \"\"\"Generates a frame visualizing the magnitude of state change (activity).\"\"\"\n",
    "    state_real = state.x_real.detach()\n",
    "    state_imag = state.x_imag.detach()\n",
    "    prev_state_real = prev_state.x_real.detach().to(DEVICE)\n",
    "    prev_state_imag = prev_state.x_imag.detach().to(DEVICE)\n",
    "    diff_real = state_real - prev_state_real\n",
    "    diff_imag = state_imag - prev_state_imag\n",
    "    change_magnitude_sq = diff_real.pow(2) + diff_imag.pow(2)\n",
    "    change_magnitude_map = torch.sqrt(change_magnitude_sq.squeeze(0).sum(dim=2) + 1e-8)\n",
    "    m_min, m_max = change_magnitude_map.min(), change_magnitude_map.max()\n",
    "    norm_factor = m_max - m_min\n",
    "    if norm_factor < 1e-12:\n",
    "        normalized_change = torch.zeros_like(change_magnitude_map).to(DEVICE)\n",
    "    else:\n",
    "        normalized_change = (change_magnitude_map - m_min) / norm_factor # Normalized\n",
    "    normalized_change_clamped = normalized_change.clamp(0.0, 1.0)\n",
    "    img_gray = normalized_change_clamped\n",
    "    img_rgb = torch.stack([img_gray, img_gray, img_gray], dim=2)\n",
    "    final_image = (img_rgb * 255).byte().cpu().numpy()\n",
    "    return final_image\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# 4.2: State Checkpointing Functions\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def load_qca_state(motor_instance, checkpoint_filepath):\n",
    "    \"\"\"Loads the QCA state (x_real, x_imag) from a checkpoint file.\"\"\"\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_filepath, map_location=motor_instance.state.x_real.device)\n",
    "        if 'x_real' in checkpoint and 'x_imag' in checkpoint and \\\n",
    "           checkpoint['x_real'].shape == motor_instance.state.x_real.shape and \\\n",
    "           checkpoint['x_imag'].shape == motor_instance.state.x_imag.shape:\n",
    "\n",
    "            motor_instance.state.x_real.data = checkpoint['x_real'].data.to(motor_instance.state.x_real.device)\n",
    "            motor_instance.state.x_imag.data = checkpoint['x_imag'].data.to(motor_instance.state.x_imag.device)\n",
    "            print(f\"‚úÖ State loaded successfully from: {checkpoint_filepath}\")\n",
    "            return checkpoint.get('step', -1)\n",
    "        else:\n",
    "            print(\"‚ùå Error loading state: Checkpoint file invalid or dimensions mismatch.\")\n",
    "            return -1\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error loading state: File '{checkpoint_filepath}' not found.\")\n",
    "        return -1\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading state from '{checkpoint_filepath}': {e}\")\n",
    "        return -1\n",
    "\n",
    "def save_qca_state(motor_instance, step, checkpoint_dir):\n",
    "    \"\"\"Saves the current QCA state (x_real, x_imag) and step number.\"\"\"\n",
    "    checkpoint_filename = os.path.join(\n",
    "        checkpoint_dir,\n",
    "        f\"large_sim_state_step_{step}.pth\"\n",
    "    )\n",
    "    try:\n",
    "        torch.save({\n",
    "            'step': step,\n",
    "            'x_real': motor_instance.state.x_real.data.cpu(),\n",
    "            'x_imag': motor_instance.state.x_imag.data.cpu()\n",
    "        }, checkpoint_filename)\n",
    "        print(f\"\\nüíæ Large simulation checkpoint saved: step {step}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error saving large simulation checkpoint at step {step}: {e}\")\n",
    "\n",
    "print(\"Visualization and state checkpointing functions defined.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # PHASE 2: ADVANCED TRAINER (PEF v3)\n",
    "# ---\n",
    "# Definition of the `QC_Trainer_v3` class for training the M-Law.\n",
    "# %% [code]\n",
    "# ------------------------------------------------------------------------------\n",
    "# 2.1: QC_Trainer_v3 Class\n",
    "# ------------------------------------------------------------------------------\n",
    "class QC_Trainer_v3:\n",
    "    def __init__(self, motor, lr_rate):\n",
    "        self.motor = motor\n",
    "        if isinstance(self.motor.operator, nn.DataParallel):\n",
    "            params_to_optimize = self.motor.operator.module.parameters()\n",
    "        else:\n",
    "            params_to_optimize = self.motor.operator.parameters()\n",
    "\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            params_to_optimize,\n",
    "            lr=lr_rate,\n",
    "            weight_decay=1e-6,\n",
    "            betas=(0.9, 0.999)\n",
    "        )\n",
    "\n",
    "        self.history = {\n",
    "            'Loss': [],\n",
    "            'R_Density_Target': [],\n",
    "            'R_Causalidad': [],\n",
    "            'R_Stability': [],\n",
    "            'P_Explosion': [],\n",
    "            'Gradient_Norm': [],\n",
    "            'R_Activity_Var': [],\n",
    "            'R_Velocidad': []\n",
    "        }\n",
    "\n",
    "        self.current_episode = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.stagnation_counter = 0\n",
    "        self.reactivation_counter = 0\n",
    "        self.gradient_norms = []\n",
    "\n",
    "    def _calculate_annealed_alpha_gamma(self, total_episodes):\n",
    "        \"\"\"Calculates annealed weights for Alpha and Gamma.\"\"\"\n",
    "        total_episodes_for_annealing = total_episodes * 0.75\n",
    "        progress = min(1.0, self.current_episode / max(1.0, total_episodes_for_annealing))\n",
    "        alpha_progress = 1 - (1 - progress) ** 1.5\n",
    "        gamma_progress = progress ** 0.7\n",
    "        current_alpha = ALPHA_START + (ALPHA_END - ALPHA_START) * alpha_progress\n",
    "        current_gamma = GAMMA_START + (GAMMA_END - GAMMA_START) * gamma_progress\n",
    "        return current_alpha, current_gamma\n",
    "\n",
    "    def _save_checkpoint(self, episode, is_best=False):\n",
    "        \"\"\"Saves the training state to a .pth file.\"\"\"\n",
    "        if is_best:\n",
    "             filename = f\"{CHECKPOINT_DIR}/qca_best_eps{episode}.pth\"\n",
    "        else:\n",
    "             filename = f\"{CHECKPOINT_DIR}/qca_checkpoint_eps{episode}.pth\"\n",
    "\n",
    "        if isinstance(self.motor.operator, nn.DataParallel):\n",
    "            model_state_dict = self.motor.operator.module.state_dict()\n",
    "        else:\n",
    "            model_state_dict = self.motor.operator.state_dict()\n",
    "\n",
    "        state = {\n",
    "            'episode': episode,\n",
    "            'model_state_dict': model_state_dict,\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'best_loss': self.best_loss,\n",
    "            'stagnation_counter': self.stagnation_counter,\n",
    "            'reactivation_counter': self.reactivation_counter,\n",
    "            'history': self.history\n",
    "        }\n",
    "        torch.save(state, filename)\n",
    "        print(f\"\\n[Checkpoint saved to: {filename}]\")\n",
    "\n",
    "    def _load_checkpoint(self):\n",
    "        \"\"\"Loads the latest training checkpoint from CHECKPOINT_DIR.\"\"\"\n",
    "        try:\n",
    "            list_of_files = glob.glob(f\"{CHECKPOINT_DIR}/qca_checkpoint_eps*.pth\")\n",
    "            list_of_best_files = glob.glob(f\"{CHECKPOINT_DIR}/qca_best_eps*.pth\")\n",
    "            all_checkpoint_files = list_of_files + list_of_best_files\n",
    "\n",
    "            if not all_checkpoint_files:\n",
    "                print(\"No checkpoints found. Starting from scratch.\")\n",
    "                return\n",
    "\n",
    "            latest_file = max(all_checkpoint_files, key=os.path.getmtime)\n",
    "            checkpoint = torch.load(latest_file, map_location=DEVICE)\n",
    "\n",
    "            if isinstance(self.motor.operator, nn.DataParallel):\n",
    "                 self.motor.operator.module.load_state_dict(checkpoint['model_state_dict'])\n",
    "            else:\n",
    "                first_key = next(iter(checkpoint['model_state_dict']))\n",
    "                if first_key.startswith('module.'):\n",
    "                    new_state_dict = {k.replace('module.', ''): v for k, v in checkpoint['model_state_dict'].items()}\n",
    "                    self.motor.operator.load_state_dict(new_state_dict)\n",
    "                else:\n",
    "                    self.motor.operator.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "            self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "            self.current_episode = checkpoint['episode'] + 1\n",
    "            self.best_loss = checkpoint['best_loss']\n",
    "\n",
    "            loaded_history = checkpoint.get('history', {})\n",
    "            for key in self.history.keys():\n",
    "                self.history[key] = loaded_history.get(key, []) # Init missing keys\n",
    "\n",
    "            self.stagnation_counter = checkpoint.get('stagnation_counter', 0)\n",
    "            self.reactivation_counter = checkpoint.get('reactivation_counter', 0)\n",
    "\n",
    "            print(f\"Checkpoint loaded: {latest_file}. Resuming from episode {self.current_episode}.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n",
    "            self.current_episode = 0\n",
    "            self.history = {k: [] for k in self.history.keys()}\n",
    "            self.best_loss = float('inf')\n",
    "            self.stagnation_counter = 0\n",
    "            self.reactivation_counter = 0\n",
    "            self.gradient_norms = []\n",
    "\n",
    "    def check_stagnation_and_reactivate(self, total_episodes):\n",
    "        \"\"\"Checks for training stagnation and triggers reactivation if configured.\"\"\"\n",
    "        current_loss = self.history['Loss'][-1] if self.history['Loss'] else float('inf')\n",
    "\n",
    "        if current_loss < (self.best_loss - MIN_LOSS_IMPROVEMENT):\n",
    "            self.best_loss = current_loss\n",
    "            self.stagnation_counter = 0\n",
    "            return False # Not stagnated\n",
    "\n",
    "        else:\n",
    "            self.stagnation_counter += 1\n",
    "            if self.stagnation_counter >= STAGNATION_WINDOW:\n",
    "                print(f\"\\nSTAGNATION DETECTED at episode {self.current_episode}!\")\n",
    "                print(f\"No improvement of {MIN_LOSS_IMPROVEMENT} in {STAGNATION_WINDOW} episodes.\")\n",
    "\n",
    "                if self.reactivation_counter < REACTIVATION_COUNT:\n",
    "                    self.reactivation_counter += 1\n",
    "                    print(f\"Attempting reactivation {self.reactivation_counter}/{REACTIVATION_COUNT}...\")\n",
    "\n",
    "                    if REACTIVATION_STATE_MODE == 'random':\n",
    "                        self.motor.state._reset_state_random()\n",
    "                        print(\"-> Resetting state with random noise.\")\n",
    "                    elif REACTIVATION_STATE_MODE == 'seeded':\n",
    "                         self.motor.state._reset_state_seeded()\n",
    "                         print(\"-> Resetting state with central seed.\")\n",
    "                    elif REACTIVATION_STATE_MODE == 'complex_noise':\n",
    "                         self.motor.state._reset_state_complex_noise()\n",
    "                         print(\"-> Resetting state with complex noise.\")\n",
    "                    else:\n",
    "                         print(f\"State reactivation mode '{REACTIVATION_STATE_MODE}' not recognized. Resetting to random.\")\n",
    "                         self.motor.state._reset_state_random()\n",
    "\n",
    "                    current_lr = self.optimizer.param_groups[0]['lr']\n",
    "                    new_lr = current_lr * REACTIVATION_LR_MULTIPLIER\n",
    "                    for param_group in self.optimizer.param_groups:\n",
    "                        param_group['lr'] = new_lr\n",
    "                    print(f\"-> Learning rate adjusted from {current_lr:.2e} to {new_lr:.2e}.\")\n",
    "\n",
    "                    self.stagnation_counter = 0\n",
    "                    print(\"-> Reactivation complete. Continuing training.\")\n",
    "                    return False # Do not stop\n",
    "\n",
    "                else:\n",
    "                    print(f\"Maximum number of reactivations ({REACTIVATION_COUNT}) reached.\")\n",
    "                    return True # Stop training\n",
    "\n",
    "        return False # Not stagnated yet\n",
    "\n",
    "    def train_episode(self, total_episodes):\n",
    "        \"\"\"Runs one full training episode (BPTT-k).\"\"\"\n",
    "        self.motor.state._reset_state_random()\n",
    "        alpha, gamma = self._calculate_annealed_alpha_gamma(total_episodes)\n",
    "\n",
    "        episode_total_loss = 0.0\n",
    "        bptt_cumulative_loss = 0.0\n",
    "        valid_steps = 0\n",
    "        current_real = self.motor.state.x_real.clone().requires_grad_(True).to(DEVICE)\n",
    "        current_imag = self.motor.state.x_imag.clone().requires_grad_(True).to(DEVICE)\n",
    "\n",
    "        activity_variances_per_step_mean = []\n",
    "        density_variances_per_step = []\n",
    "\n",
    "        for t in range(STEPS_PER_EPISODE):\n",
    "            if torch.isnan(current_real).any() or torch.isinf(current_real).any() or \\\n",
    "               torch.isnan(current_imag).any() or torch.isinf(current_imag).any():\n",
    "                print(f\"‚ö†Ô∏è  NaN/Inf detected in state at step {t} of episode {self.current_episode}.\")\n",
    "                episode_total_loss = float('nan')\n",
    "                break\n",
    "\n",
    "            prev_real_detached = current_real.detach()\n",
    "            prev_imag_detached = current_imag.detach()\n",
    "\n",
    "            x_real_c = current_real.permute(0, 3, 1, 2)\n",
    "            x_imag_c = current_imag.permute(0, 3, 1, 2)\n",
    "            x_cat = torch.cat([x_real_c, x_imag_c], dim=1).to(DEVICE)\n",
    "\n",
    "            if isinstance(self.motor.operator, nn.DataParallel):\n",
    "                F_int_real, F_int_imag = self.motor.operator(x_cat)\n",
    "            else:\n",
    "                F_int_real, F_int_imag = self.motor.operator(x_cat)\n",
    "\n",
    "            if torch.isnan(F_int_real).any() or torch.isinf(F_int_real).any() or \\\n",
    "               torch.isnan(F_int_imag).any() or torch.isinf(F_int_imag).any():\n",
    "                print(f\"‚ö†Ô∏è  NaN/Inf detected in F_int at step {t} of episode {self.current_episode}.\")\n",
    "                episode_total_loss = float('nan')\n",
    "                break\n",
    "\n",
    "            if isinstance(self.motor.operator, nn.DataParallel):\n",
    "                bias_real = self.motor.operator.module.M_bias_real.to(DEVICE)\n",
    "                bias_imag = self.motor.operator.module.M_bias_imag.to(DEVICE)\n",
    "            else:\n",
    "                bias_real = self.motor.operator.M_bias_real.to(DEVICE)\n",
    "                bias_imag = self.motor.operator.M_bias_imag.to(DEVICE)\n",
    "\n",
    "            new_real = current_real.squeeze(0) + F_int_real + bias_real\n",
    "            new_imag = current_imag.squeeze(0) + F_int_imag + bias_imag\n",
    "\n",
    "            new_real = torch.clamp(new_real, -1.5, 1.5)\n",
    "            new_imag = torch.clamp(new_imag, -1.5, 1.5)\n",
    "\n",
    "            prob_sq = new_real.pow(2) + new_imag.pow(2)\n",
    "            norm = torch.sqrt(prob_sq.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "            next_real = new_real / norm\n",
    "            next_imag = new_imag / norm\n",
    "\n",
    "            if torch.isnan(next_real).any() or torch.isinf(next_real).any() or \\\n",
    "               torch.isnan(next_imag).any() or torch.isinf(next_imag).any():\n",
    "                print(f\"‚ö†Ô∏è  NaN/Inf detected in next_state at step {t} of episode {self.current_episode}.\")\n",
    "                episode_total_loss = float('nan')\n",
    "                break\n",
    "\n",
    "            density_map = torch.clamp(prob_sq.sum(dim=-1), 0.0, 3.0)\n",
    "\n",
    "            # --- Reward and Penalty Calculation ---\n",
    "            current_std_density = density_map.std()\n",
    "            density_error = torch.abs(current_std_density - TARGET_STD_DENSITY)\n",
    "            R_density_target = -density_error * (1.0 + density_error)\n",
    "\n",
    "            change_real = next_real - prev_real_detached.squeeze(0)\n",
    "            change_imag = next_imag - prev_imag_detached.squeeze(0)\n",
    "            R_Causalidad = -(change_real.abs().mean() + change_imag.abs().mean())\n",
    "\n",
    "            density_t_plus_1 = next_real.pow(2) + next_imag.pow(2)\n",
    "            R_Stability = -density_t_plus_1.var(dim=-1).mean()\n",
    "\n",
    "            change_magnitude_per_cell = torch.sqrt(change_real.pow(2) + change_imag.pow(2)).sum(dim=-1)\n",
    "            activity_variances_per_step_mean.append(change_magnitude_per_cell.mean().item())\n",
    "            density_variances_per_step.append(density_map.var().item())\n",
    "\n",
    "            P_Explosion = torch.relu(density_map.max() - EXPLOSION_THRESHOLD) * EXPLOSION_PENALTY_MULTIPLIER\n",
    "\n",
    "            # Step-wise reward/penalty (for BPTT)\n",
    "            reward_step_bptt = (alpha * R_density_target) + \\\n",
    "                               (BETA_CAUSALITY * R_Causalidad) + \\\n",
    "                               (gamma * R_Stability) + \\\n",
    "                               (LAMBDA_ACTIVITY_VAR * change_magnitude_per_cell.var()) - \\\n",
    "                               (LAMBDA_VELOCIDAD * density_map.var())\n",
    "\n",
    "            step_loss = -reward_step_bptt + P_Explosion\n",
    "\n",
    "            if torch.isnan(step_loss) or torch.isinf(step_loss):\n",
    "                 print(f\"‚ö†Ô∏è  NaN/Inf detected in step_loss at step {t} of episode {self.current_episode}.\")\n",
    "                 episode_total_loss = float('nan')\n",
    "                 break\n",
    "\n",
    "            bptt_cumulative_loss = bptt_cumulative_loss + step_loss\n",
    "            if not torch.isnan(step_loss):\n",
    "                episode_total_loss += step_loss.item()\n",
    "            valid_steps += 1\n",
    "\n",
    "            # --- Truncated Backpropagation (BPTT-k) ---\n",
    "            if (t + 1) % PERSISTENCE_COUNT == 0 or (t + 1) == STEPS_PER_EPISODE:\n",
    "                if bptt_cumulative_loss != 0 and not torch.isnan(bptt_cumulative_loss) and not torch.isinf(bptt_cumulative_loss):\n",
    "                    self.optimizer.zero_grad()\n",
    "                    bptt_cumulative_loss.backward()\n",
    "\n",
    "                    total_norm = 0.0\n",
    "                    params_to_clip = []\n",
    "                    if isinstance(self.motor.operator, nn.DataParallel):\n",
    "                        params_to_clip = [p for p in self.motor.operator.module.parameters() if p.requires_grad and p.grad is not None]\n",
    "                    else:\n",
    "                        params_to_clip = [p for p in self.motor.operator.parameters() if p.requires_grad and p.grad is not None]\n",
    "\n",
    "                    if params_to_clip:\n",
    "                         for p in params_to_clip:\n",
    "                             param_norm = p.grad.data.norm(2)\n",
    "                             total_norm += param_norm.item() ** 2\n",
    "                         total_norm = total_norm ** 0.5\n",
    "                         self.gradient_norms.append(total_norm)\n",
    "                         torch.nn.utils.clip_grad_norm_(params_to_clip, GRADIENT_CLIP)\n",
    "                    else:\n",
    "                         self.gradient_norms.append(0.0)\n",
    "\n",
    "                    self.optimizer.step()\n",
    "                else:\n",
    "                    print(f\"‚ö†Ô∏è  BPTT cumulative loss is NaN/Inf/Zero at step {t} of episode {self.current_episode}. Skipping backward pass.\")\n",
    "                    self.gradient_norms.append(0.0)\n",
    "\n",
    "                bptt_cumulative_loss = 0.0\n",
    "                current_real = next_real.unsqueeze(0).detach().to(DEVICE).requires_grad_(True)\n",
    "                current_imag = next_imag.unsqueeze(0).detach().to(DEVICE).requires_grad_(True)\n",
    "            else:\n",
    "                current_real = next_real.unsqueeze(0)\n",
    "                current_imag = next_imag.unsqueeze(0)\n",
    "\n",
    "        # --- End of Episode ---\n",
    "        episodic_R_Activity_Var = 0.0\n",
    "        if len(activity_variances_per_step_mean) > 1:\n",
    "             episodic_R_Activity_Var = float(np.var(activity_variances_per_step_mean)) * LAMBDA_ACTIVITY_VAR\n",
    "\n",
    "        episodic_R_Velocidad = 0.0\n",
    "        if len(density_variances_per_step) > 0:\n",
    "            episodic_R_Velocidad = np.mean(density_variances_per_step) * LAMBDA_VELOCIDAD\n",
    "\n",
    "        avg_loss = episode_total_loss / max(valid_steps, 1)\n",
    "\n",
    "        # --- Store Metrics in History ---\n",
    "        self.history['Loss'].append(avg_loss)\n",
    "        self.history['R_Density_Target'].append(R_density_target.item() if valid_steps > 0 and 'R_density_target' in locals() else float('nan'))\n",
    "        self.history['R_Causalidad'].append(R_Causalidad.item() if valid_steps > 0 and 'R_Causalidad' in locals() else float('nan'))\n",
    "        self.history['R_Stability'].append(R_Stability.item() if valid_steps > 0 and 'R_Stability' in locals() else float('nan'))\n",
    "        self.history['P_Explosion'].append(P_Explosion.item() if valid_steps > 0 and 'P_Explosion' in locals() else float('nan'))\n",
    "        self.history['R_Activity_Var'].append(episodic_R_Activity_Var if valid_steps > 0 else float('nan'))\n",
    "        self.history['R_Velocidad'].append(episodic_R_Velocidad if valid_steps > 0 else float('nan'))\n",
    "\n",
    "        if self.gradient_norms:\n",
    "             self.history['Gradient_Norm'].append(np.mean(self.gradient_norms))\n",
    "             self.gradient_norms = []\n",
    "        else:\n",
    "             self.history['Gradient_Norm'].append(0.0)\n",
    "\n",
    "        self.current_episode += 1\n",
    "        gc.collect()\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        return avg_loss\n",
    "\n",
    "print(\"QC_Trainer_v3 class defined.\")\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# # PHASES 5, 6 & 7: MAIN EXECUTION PIPELINE\n",
    "# ---\n",
    "# This is the main runnable part of the script.\n",
    "# It uses the flags from PHASE 3 to determine which sections to run.\n",
    "# %% [code]\n",
    "def main_pipeline():\n",
    "    \"\"\"\n",
    "    Main function to run the AETHERIA pipeline based on global flags.\n",
    "    \"\"\"\n",
    "    print(\"--- STARTING AETHERIA PIPELINE EXECUTION ---\")\n",
    "\n",
    "    # These variables need to be accessible across phases\n",
    "    Aetheria_Motor_Train = None\n",
    "    M_FILENAME = None\n",
    "    trainer = None\n",
    "    model_id = \"Deep_v3\" # Default model ID\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # FASE 5: MAIN TRAINING LOGIC\n",
    "    # --------------------------------------------------------------------------\n",
    "    if RUN_TRAINING:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\">>> STARTING TRAINING PHASE (PHASE 5) <<<\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        # Initialize the model (QCA_Operator_Deep)\n",
    "        model_M = QCA_Operator_Deep(D_STATE, HIDDEN_CHANNELS)\n",
    "\n",
    "        # Instantiate the Aetheria_Motor for the training phase.\n",
    "        Aetheria_Motor_Train = Aetheria_Motor(GRID_SIZE_TRAINING, D_STATE, model_M)\n",
    "\n",
    "        print(f\"Motor and M-Law ({model_id}) initialized. Training grid: {GRID_SIZE_TRAINING}x{GRID_SIZE_TRAINING}.\")\n",
    "\n",
    "        # Correctly get the number of trainable parameters\n",
    "        trainable_params = sum(p.numel() for p in (Aetheria_Motor_Train.operator.module.parameters()\n",
    "                                                   if isinstance(Aetheria_Motor_Train.operator, nn.DataParallel)\n",
    "                                                   else Aetheria_Motor_Train.operator.parameters()) if p.requires_grad)\n",
    "        print(f\"Trainable Parameters: {trainable_params}\")\n",
    "\n",
    "        # Instantiate the trainer\n",
    "        trainer = QC_Trainer_v3(Aetheria_Motor_Train, LR_RATE_M)\n",
    "\n",
    "        # Load checkpoint if CONTINUE_TRAINING is True.\n",
    "        if CONTINUE_TRAINING:\n",
    "            print(\"Attempting to continue training...\")\n",
    "            trainer._load_checkpoint()\n",
    "        else:\n",
    "            print(\"Starting new training from scratch.\")\n",
    "\n",
    "        # MAIN TRAINING LOOP\n",
    "        print(\"\\n--- Training (PEF v3 - Optimized Parameters + Reactivation + New Rewards) ---\")\n",
    "        print(f\"Starting from episode {trainer.current_episode}. Training for {EPISODES_TO_ADD} more episodes.\")\n",
    "        print(f\"Model: {model_id}, BPTT-k: {PERSISTENCE_COUNT}, Initial LR: {LR_RATE_M}\")\n",
    "        print(f\"Target Std Density: {TARGET_STD_DENSITY}, Explosion Threshold: {EXPLOSION_THRESHOLD}, Explosion Multiplier: {EXPLOSION_PENALTY_MULTIPLIER}\")\n",
    "        print(f\"Annealing Alpha: {ALPHA_START} -> {ALPHA_END}, Gamma: {GAMMA_START} -> {GAMMA_END}, Beta Causality: {BETA_CAUSALITY}\")\n",
    "        print(f\"New Reward Weights: Activity Var (Œª={LAMBDA_ACTIVITY_VAR}), Velocidad (Œª={LAMBDA_VELOCIDAD})\")\n",
    "        print(f\"Gradient Clip: {GRADIENT_CLIP}, Stagnation Window: {STAGNATION_WINDOW}, Min Loss Improvement: {MIN_LOSS_IMPROVEMENT}\")\n",
    "        print(f\"Reactivation: {REACTIVATION_COUNT} attempts with state '{REACTIVATION_STATE_MODE}' and LR * {REACTIVATION_LR_MULTIPLIER}\")\n",
    "        print(f\"Save Checkpoint every {SAVE_EVERY_EPISODES} episodes.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        final_episode = trainer.current_episode + EPISODES_TO_ADD\n",
    "\n",
    "        try:\n",
    "            for episode in range(trainer.current_episode, final_episode):\n",
    "                avg_loss = trainer.train_episode(final_episode)\n",
    "\n",
    "                if np.isnan(avg_loss) or np.isinf(avg_loss):\n",
    "                    print(f\"‚ö†Ô∏è  Episode {episode:04}: Training failed (NaN/Inf in loss). Skipping save and continuing.\")\n",
    "                    Aetheria_Motor_Train.state._reset_state_random()\n",
    "                    continue\n",
    "\n",
    "                # Print progress\n",
    "                if episode % 10 == 0 or episode == final_episode - 1 or episode == trainer.current_episode:\n",
    "                    alpha, gamma = trainer._calculate_annealed_alpha_gamma(final_episode)\n",
    "                    last_r_density = trainer.history['R_Density_Target'][-1] if trainer.history['R_Density_Target'] else float('nan')\n",
    "                    last_r_causalidad = trainer.history['R_Causalidad'][-1] if trainer.history['R_Causalidad'] else float('nan')\n",
    "                    last_r_stability = trainer.history['R_Stability'][-1] if trainer.history['R_Stability'] else float('nan')\n",
    "                    last_p_explosion = trainer.history['P_Explosion'][-1] if trainer.history['P_Explosion'] else float('nan')\n",
    "                    last_grad_norm = trainer.history['Gradient_Norm'][-1] if trainer.history['Gradient_Norm'] else float('nan')\n",
    "                    last_r_activity_var = trainer.history['R_Activity_Var'][-1] if trainer.history['R_Activity_Var'] else float('nan')\n",
    "                    last_r_velocidad = trainer.history['R_Velocidad'][-1] if trainer.history['R_Velocidad'] else float('nan')\n",
    "\n",
    "                    print(f\"Eps {episode:04}: Loss={avg_loss:.3e} | \"\n",
    "                          f\"R_Dens(Tgt={TARGET_STD_DENSITY:.2f})={last_r_density:.3f} | \"\n",
    "                          f\"R_Caus={last_r_causalidad:.3f} | \"\n",
    "                          f\"R_Stab={last_r_stability:.3f} | \"\n",
    "                          f\"R_ActVar={last_r_activity_var:.3e} | \"\n",
    "                          f\"R_Vel={last_r_velocidad:.3f} | \"\n",
    "                          f\"P_Expl={last_p_explosion:.3e} | \"\n",
    "                          f\"GradNorm={last_grad_norm:.3e} | \"\n",
    "                          f\"Œ±={alpha:.2f}, Œ≥={gamma:.2f}, LR={trainer.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "\n",
    "                # Save checkpoint\n",
    "                if episode % SAVE_EVERY_EPISODES == 0 and episode > (trainer.current_episode - EPISODES_TO_ADD): # Avoid saving at start if continuing\n",
    "                    trainer._save_checkpoint(episode)\n",
    "                    if not np.isnan(avg_loss) and not np.isinf(avg_loss) and avg_loss < trainer.best_loss:\n",
    "                        trainer._save_checkpoint(episode, is_best=True)\n",
    "                        print(f\"üèÜ New best model saved at episode {episode}\")\n",
    "\n",
    "                # Check for stagnation\n",
    "                if trainer.check_stagnation_and_reactivate(final_episode):\n",
    "                    print(\"Training stopped due to stagnation with no reactivations left.\")\n",
    "                    break\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nTraining interrupted by user. Saving checkpoint...\")\n",
    "            if trainer: trainer._save_checkpoint(trainer.current_episode)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Critical error during training at episode {trainer.current_episode if trainer else 'N/A'}: {e}\")\n",
    "            print(\"Saving current state before stopping...\")\n",
    "            if trainer: trainer._save_checkpoint(trainer.current_episode)\n",
    "            raise e # Re-raise error\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"\\nTraining completed in {end_time - start_time:.2f} seconds.\")\n",
    "        if trainer: print(f\"Final episode reached: {trainer.current_episode}\")\n",
    "\n",
    "        # SAVE FINAL MODEL (M Law)\n",
    "        print(\"\\n--- SAVING FINAL FUNDAMENTAL LAW (M) ---\")\n",
    "        TIMESTAMP = int(time.time())\n",
    "        if trainer:\n",
    "            M_FILENAME = f\"{CHECKPOINT_DIR}/PEF_{model_id}_G{GRID_SIZE_TRAINING}_Eps{trainer.current_episode}_{TIMESTAMP}_FINAL.pth\"\n",
    "            try:\n",
    "                if isinstance(trainer.motor.operator, nn.DataParallel):\n",
    "                    model_state_dict_to_save = trainer.motor.operator.module.state_dict()\n",
    "                else:\n",
    "                    model_state_dict_to_save = trainer.motor.operator.state_dict()\n",
    "                torch.save(model_state_dict_to_save, M_FILENAME)\n",
    "                print(f\"‚úÖ Final Fundamental Law (M) saved: {M_FILENAME}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error saving final model: {e}\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n>>> TRAINING PHASE (PHASE 5) SKIPPED <<<\")\n",
    "        # If training is skipped, we still need to initialize the training motor\n",
    "        # and load the model if we plan to run visualization or large simulation.\n",
    "        if RUN_POST_TRAINING_VIZ or RUN_LARGE_SIM:\n",
    "            print(\"Initializing training-size motor to load model...\")\n",
    "            model_M = QCA_Operator_Deep(D_STATE, HIDDEN_CHANNELS)\n",
    "            Aetheria_Motor_Train = Aetheria_Motor(GRID_SIZE_TRAINING, D_STATE, model_M)\n",
    "\n",
    "            # Attempt to load the most recent FINAL model\n",
    "            model_files = glob.glob(f\"{CHECKPOINT_DIR}/PEF_Deep_v3_G{GRID_SIZE_TRAINING}_Eps*_FINAL.pth\")\n",
    "            M_FILENAME = max(model_files, key=os.path.getctime, default=None) if model_files else None\n",
    "\n",
    "            if M_FILENAME and os.path.exists(M_FILENAME):\n",
    "                print(f\"üì¶ Detected latest .pth file (assumed trained model): {M_FILENAME}. Attempting to load weights...\")\n",
    "                try:\n",
    "                    model_state_dict = torch.load(M_FILENAME, map_location=DEVICE)\n",
    "                    if isinstance(Aetheria_Motor_Train.operator, nn.DataParallel):\n",
    "                        Aetheria_Motor_Train.operator.module.load_state_dict(model_state_dict)\n",
    "                    else:\n",
    "                        first_key = next(iter(model_state_dict))\n",
    "                        if first_key.startswith('module.'):\n",
    "                            new_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}\n",
    "                            Aetheria_Motor_Train.operator.load_state_dict(new_state_dict)\n",
    "                        else:\n",
    "                            Aetheria_Motor_Train.operator.load_state_dict(model_state_dict)\n",
    "                    Aetheria_Motor_Train.operator.eval()\n",
    "                    print(\"‚úÖ Model weights loaded successfully into Aetheria_Motor_Train.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error loading model weights '{M_FILENAME}': {e}\")\n",
    "                    print(\"‚ö†Ô∏è  Could not load trained model. Visualization/simulation may not work as expected.\")\n",
    "            else:\n",
    "                print(f\"‚ùå No PEF_Deep_v3_G{GRID_SIZE_TRAINING}_Eps*_FINAL.pth files found in '{CHECKPOINT_DIR}'. No trained model will be loaded.\")\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # FASE 6: POST-TRAINING VISUALIZATION (Training Size)\n",
    "    # --------------------------------------------------------------------------\n",
    "    if RUN_POST_TRAINING_VIZ:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\">>> STARTING POST-TRAINING VISUALIZATION PHASE (PHASE 6) <<<\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        if Aetheria_Motor_Train is not None: # Ensure the training motor exists and model is loaded\n",
    "            print(f\"Generating {NUM_FRAMES_VIZ} frames with the M-Law on {GRID_SIZE_TRAINING}x{GRID_SIZE_TRAINING} grid...\")\n",
    "\n",
    "            Aetheria_Motor_Train.operator.eval()\n",
    "            Aetheria_Motor_Train.state._reset_state_random() # Start from a fresh random state\n",
    "            NUM_FRAMES_VIZ_TRAINING = NUM_FRAMES_VIZ # Use the parameter from the top\n",
    "\n",
    "            FRAMES_DENSITY_TRAINING = []\n",
    "            FRAMES_CHANNELS_TRAINING = []\n",
    "            FRAMES_MAGNITUDE_TRAINING = []\n",
    "            FRAMES_PHASE_TRAINING = []\n",
    "            FRAMES_CHANGE_TRAINING = []\n",
    "\n",
    "            prev_state_for_change_viz_training = QCA_State(Aetheria_Motor_Train.size, Aetheria_Motor_Train.d_state)\n",
    "            prev_state_for_change_viz_training.x_real.data = Aetheria_Motor_Train.state.x_real.data.clone()\n",
    "            prev_state_for_change_viz_training.x_imag.data = Aetheria_Motor_Train.state.x_imag.data.clone()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for t in range(NUM_FRAMES_VIZ_TRAINING):\n",
    "                    # Store current state *before* evolution for change calculation\n",
    "                    current_state_clone_for_change_viz_training = QCA_State(Aetheria_Motor_Train.state.size, Aetheria_Motor_Train.state.d_state)\n",
    "                    current_state_clone_for_change_viz_training.x_real.data = Aetheria_Motor_Train.state.x_real.data.clone().to(DEVICE)\n",
    "                    current_state_clone_for_change_viz_training.x_imag.data = Aetheria_Motor_Train.state.x_imag.data.clone().to(DEVICE)\n",
    "\n",
    "                    Aetheria_Motor_Train.evolve_step()\n",
    "                    next_state = Aetheria_Motor_Train.state\n",
    "\n",
    "                    # Generate frames\n",
    "                    FRAMES_DENSITY_TRAINING.append(get_density_frame_gpu(next_state))\n",
    "                    FRAMES_CHANNELS_TRAINING.append(get_channel_frame_gpu(next_state, num_channels=min(3, D_STATE)))\n",
    "                    FRAMES_MAGNITUDE_TRAINING.append(get_state_magnitude_frame_gpu(next_state))\n",
    "                    FRAMES_PHASE_TRAINING.append(get_state_phase_frame_gpu(next_state))\n",
    "                    FRAMES_CHANGE_TRAINING.append(get_state_change_magnitude_frame_gpu(next_state, prev_state_for_change_viz_training))\n",
    "\n",
    "                    # Update previous state\n",
    "                    prev_state_for_change_viz_training = current_state_clone_for_change_viz_training\n",
    "\n",
    "                    if (t + 1) % max(1, (NUM_FRAMES_VIZ_TRAINING // 10)) == 0:\n",
    "                        print(f\"-> Capturing frame {t+1}/{NUM_FRAMES_VIZ_TRAINING}...\")\n",
    "\n",
    "            print(\"‚úÖ Visualization frame capture completed.\")\n",
    "\n",
    "            print(\"\\n--- SAVING VISUALIZATION VIDEOS (Training Size) ---\")\n",
    "            try:\n",
    "                # Use the filename from the final saved model to name the visualization videos.\n",
    "                if M_FILENAME and os.path.exists(M_FILENAME):\n",
    "                    BASE_FILENAME_VIZ_TRAINING = os.path.basename(M_FILENAME).replace('_FINAL.pth', '')\n",
    "                elif trainer is not None and hasattr(trainer, 'current_episode'):\n",
    "                    TIMESTAMP_VIZ = int(time.time())\n",
    "                    BASE_FILENAME_VIZ_TRAINING = f\"Viz_TrainSize_G{GRID_SIZE_TRAINING}_Eps{trainer.current_episode}_{TIMESTAMP_VIZ}\"\n",
    "                    print(f\"‚ö†Ô∏è M_FILENAME not available. Using default base name: {BASE_FILENAME_VIZ_TRAINING}\")\n",
    "                else:\n",
    "                    TIMESTAMP_VIZ = int(time.time())\n",
    "                    BASE_FILENAME_VIZ_TRAINING = f\"Viz_TrainSize_G{GRID_SIZE_TRAINING}_{TIMESTAMP_VIZ}\"\n",
    "                    print(f\"‚ö†Ô∏è M_FILENAME and trainer not available. Using default base name: {BASE_FILENAME_VIZ_TRAINING}\")\n",
    "\n",
    "                MP4_DENSITY_FILENAME_TRAINING = f\"{BASE_FILENAME_VIZ_TRAINING}_1_DENSITY.mp4\"\n",
    "                MP4_CHANNELS_FILENAME_TRAINING = f\"{BASE_FILENAME_VIZ_TRAINING}_2_CHANNELS.mp4\"\n",
    "                MP4_MAGNITUDE_FILENAME_TRAINING = f\"{BASE_FILENAME_VIZ_TRAINING}_3_MAGNITUDE.mp4\"\n",
    "                MP4_PHASE_FILENAME_TRAINING = f\"{BASE_FILENAME_VIZ_TRAINING}_4_PHASE.mp4\"\n",
    "                MP4_CHANGE_FILENAME_TRAINING = f\"{BASE_FILENAME_VIZ_TRAINING}_5_CHANGE.mp4\"\n",
    "\n",
    "                if FRAMES_DENSITY_TRAINING:\n",
    "                    try:\n",
    "                        imageio.mimsave(MP4_DENSITY_FILENAME_TRAINING, FRAMES_DENSITY_TRAINING, fps=FPS_VIZ_TRAINING, codec='libx264', quality=8)\n",
    "                        imageio.mimsave(MP4_CHANNELS_FILENAME_TRAINING, FRAMES_CHANNELS_TRAINING, fps=FPS_VIZ_TRAINING, codec='libx264', quality=8)\n",
    "                        imageio.mimsave(MP4_MAGNITUDE_FILENAME_TRAINING, FRAMES_MAGNITUDE_TRAINING, fps=FPS_VIZ_TRAINING, codec='libx264', quality=8)\n",
    "                        imageio.mimsave(MP4_PHASE_FILENAME_TRAINING, FRAMES_PHASE_TRAINING, fps=FPS_VIZ_TRAINING, codec='libx264', quality=8)\n",
    "                        if FRAMES_CHANGE_TRAINING:\n",
    "                             imageio.mimsave(MP4_CHANGE_FILENAME_TRAINING, FRAMES_CHANGE_TRAINING, fps=FPS_VIZ_TRAINING, codec='libx264', quality=8)\n",
    "                        print(f\"‚úÖ MP4 Videos ({GRID_SIZE_TRAINING}x{GRID_SIZE_TRAINING}) saved.\")\n",
    "                        print(f\"   -> {MP4_DENSITY_FILENAME_TRAINING}\")\n",
    "                        print(f\"   -> {MP4_CHANNELS_FILENAME_TRAINING}\")\n",
    "                        # (IPython.display.Video calls removed)\n",
    "                    except Exception as e:\n",
    "                        print(f\"‚ùå Error saving visualization videos (training size): {e}\")\n",
    "                else:\n",
    "                    print(\"‚ùå Error: No frames were generated for post-training visualization.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå General error in post-training visualization video saving: {e}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Aetheria_Motor_Train was not initialized. Skipping post-training visualization.\")\n",
    "\n",
    "        print(\"\\n>>> POST-TRAINING VISUALIZATION PHASE (PHASE 6) COMPLETED <<<\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n>>> POST-TRAINING VISUALIZATION PHASE (PHASE 6) SKIPPED <<<\")\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # FASE 7: MAIN PROLONGED LARGE SIMULATION LOGIC\n",
    "    # --------------------------------------------------------------------------\n",
    "    if RUN_LARGE_SIM:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\">>> STARTING PROLONGED LARGE SIMULATION PHASE (PHASE 7) <<<\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        print(f\"\\n--- CONFIGURING LARGE SIMULATION ({GRID_SIZE_INFERENCE}x{GRID_SIZE_INFERENCE}) ---\")\n",
    "        print(f\"Using D_STATE={D_STATE}, HIDDEN_CHANNELS={HIDDEN_CHANNELS} (from optimized training)\")\n",
    "\n",
    "        # Instantiate the operator model\n",
    "        operator_model_inference = QCA_Operator_Deep(\n",
    "            d_state=D_STATE,\n",
    "            hidden_channels=HIDDEN_CHANNELS\n",
    "        )\n",
    "\n",
    "        # Instantiate the Aetheria_Motor for the large simulation.\n",
    "        large_scale_motor = Aetheria_Motor(GRID_SIZE_INFERENCE, D_STATE, operator_model_inference)\n",
    "\n",
    "        # --- Load Trained Model Weights ---\n",
    "        if not M_FILENAME: # If M_FILENAME is not set (e.g., training skipped and no file found)\n",
    "            model_files = glob.glob(f\"{CHECKPOINT_DIR}/PEF_Deep_v3_G{GRID_SIZE_TRAINING}_Eps*_FINAL.pth\")\n",
    "            M_FILENAME = max(model_files, key=os.path.getctime, default=None) if model_files else None\n",
    "\n",
    "        if M_FILENAME and os.path.exists(M_FILENAME):\n",
    "            print(f\"üì¶ Loading weights from: {M_FILENAME}\")\n",
    "            try:\n",
    "                model_state_dict = torch.load(M_FILENAME, map_location=DEVICE)\n",
    "                if isinstance(large_scale_motor.operator, nn.DataParallel):\n",
    "                    large_scale_motor.operator.module.load_state_dict(model_state_dict)\n",
    "                else:\n",
    "                    first_key = next(iter(model_state_dict))\n",
    "                    if first_key.startswith('module.'):\n",
    "                        new_state_dict = {k.replace('module.', ''): v for k, v in model_state_dict.items()}\n",
    "                        large_scale_motor.operator.load_state_dict(new_state_dict)\n",
    "                    else:\n",
    "                        large_scale_motor.operator.load_state_dict(model_state_dict)\n",
    "                large_scale_motor.operator.eval()\n",
    "                print(\"‚úÖ Model weights loaded successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error loading model weights '{M_FILENAME}': {e}\")\n",
    "                print(\"‚ö†Ô∏è  Simulation will run with random M-Law weights.\")\n",
    "        else:\n",
    "            print(f\"‚ùå No trained model file found in '{CHECKPOINT_DIR}'. Simulation will run with random M-Law weights.\")\n",
    "\n",
    "        # --- Load state from a checkpoint or start from scratch ---\n",
    "        start_step = 0\n",
    "        latest_checkpoint_filepath = None\n",
    "\n",
    "        if LOAD_STATE_CHECKPOINT_INFERENCE:\n",
    "            if STATE_CHECKPOINT_PATH_INFERENCE and os.path.exists(STATE_CHECKPOINT_PATH_INFERENCE):\n",
    "                latest_checkpoint_filepath = STATE_CHECKPOINT_PATH_INFERENCE\n",
    "                print(f\"\\nAttempting to load state from specified checkpoint: {latest_checkpoint_filepath}\")\n",
    "            else:\n",
    "                if STATE_CHECKPOINT_PATH_INFERENCE:\n",
    "                     print(f\"‚ö†Ô∏è Specified checkpoint '{STATE_CHECKPOINT_PATH_INFERENCE}' not found. Searching for latest...\")\n",
    "                checkpoint_files = [f for f in os.listdir(LARGE_SIM_CHECKPOINT_DIR) if f.startswith(\"large_sim_state_step_\") and f.endswith(\".pth\")]\n",
    "                if checkpoint_files:\n",
    "                    def extract_step(filename):\n",
    "                        match = re.search(r\"large_sim_state_step_(\\d+)\\.pth\", filename)\n",
    "                        return int(match.group(1)) if match else 0\n",
    "                    checkpoint_files.sort(key=extract_step)\n",
    "                    latest_checkpoint_filename = checkpoint_files[-1]\n",
    "                    latest_checkpoint_filepath = os.path.join(LARGE_SIM_CHECKPOINT_DIR, latest_checkpoint_filename)\n",
    "                    print(f\"\\nDetected latest large sim checkpoint in '{LARGE_SIM_CHECKPOINT_DIR}': {latest_checkpoint_filepath}. Attempting to load...\")\n",
    "                else:\n",
    "                    print(f\"\\nLOAD_STATE_CHECKPOINT_INFERENCE is True but no large simulation checkpoints were found in '{LARGE_SIM_CHECKPOINT_DIR}'.\")\n",
    "\n",
    "            if latest_checkpoint_filepath and os.path.exists(latest_checkpoint_filepath):\n",
    "                loaded_step = load_qca_state(large_scale_motor, latest_checkpoint_filepath)\n",
    "                if loaded_step != -1:\n",
    "                    start_step = loaded_step\n",
    "                    print(f\"Simulation resumed from step {start_step}.\")\n",
    "                else:\n",
    "                    print(\"‚ùå Failed to load state checkpoint. Starting new simulation.\")\n",
    "                    start_step = 0\n",
    "            else:\n",
    "                 print(\"\\nNo large simulation checkpoint found to load. Starting new simulation.\")\n",
    "                 start_step = 0\n",
    "\n",
    "        if start_step == 0:\n",
    "            print(f\"\\nStarting new simulation with initial state mode: '{INITIAL_STATE_MODE_INFERENCE}'.\")\n",
    "            if INITIAL_STATE_MODE_INFERENCE == 'random':\n",
    "                large_scale_motor.state._reset_state_random()\n",
    "            elif INITIAL_STATE_MODE_INFERENCE == 'seeded':\n",
    "                large_scale_motor.state._reset_state_seeded()\n",
    "            elif INITIAL_STATE_MODE_INFERENCE == 'complex_noise':\n",
    "                large_scale_motor.state._reset_state_complex_noise()\n",
    "            else:\n",
    "                print(f\"Initial state mode '{INITIAL_STATE_MODE_INFERENCE}' not recognized. Defaulting to random.\")\n",
    "                large_scale_motor.state._reset_state_random()\n",
    "\n",
    "        # ----------------------------------------------------------------------\n",
    "        # Main Large Simulation Loop\n",
    "        # ----------------------------------------------------------------------\n",
    "\n",
    "        TIMESTAMP_SIM = int(time.time())\n",
    "        GRID_SIZE_STR = str(GRID_SIZE_INFERENCE)\n",
    "        SIMULATION_ID = f\"{GRID_SIZE_STR}_{TIMESTAMP_SIM}_{INITIAL_STATE_MODE_INFERENCE}\"\n",
    "        if VIDEO_SAVE_INTERVAL_STEPS is not None and VIDEO_SAVE_INTERVAL_STEPS > 0 and VIDEO_DOWNSCALE_FACTOR > 1:\n",
    "            SIMULATION_ID += f\"_down{VIDEO_DOWNSCALE_FACTOR}\"\n",
    "\n",
    "        MP4_DENSITY_FILENAME = f\"MUNDO_DENSITY_{SIMULATION_ID}.mp4\"\n",
    "        MP4_CHANNELS_FILENAME = f\"MUNDO_CHANNELS_{SIMULATION_ID}.mp4\"\n",
    "        MP4_MAGNITUDE_FILENAME = f\"MUNDO_MAGNITUDE_{SIMULATION_ID}.mp4\"\n",
    "        MP4_PHASE_FILENAME = f\"MUNDO_PHASE_{SIMULATION_ID}.mp4\"\n",
    "        MP4_CHANGE_FILENAME = f\"MUNDO_CHANGE_{SIMULATION_ID}.mp4\"\n",
    "\n",
    "        print(f\"\\nüé¨ Starting simulation of {NUM_INFERENCE_STEPS} steps on {GRID_SIZE_INFERENCE}x{GRID_SIZE_INFERENCE} from step {start_step}...\")\n",
    "        if VIDEO_SAVE_INTERVAL_STEPS is not None and VIDEO_SAVE_INTERVAL_STEPS > 0:\n",
    "            print(f\"üìπ Videos will be saved as MUNDO_*_{SIMULATION_ID}.mp4 (Frame every {VIDEO_SAVE_INTERVAL_STEPS} steps @ {VIDEO_FPS} FPS, Downscale: {VIDEO_DOWNSCALE_FACTOR}, Quality: {VIDEO_QUALITY})\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è Video saving is disabled.\")\n",
    "        if REAL_TIME_VIZ_INTERVAL is not None and REAL_TIME_VIZ_INTERVAL > 0:\n",
    "             print(f\"üëÅÔ∏è Real-time visualization enabled (Frame every {REAL_TIME_VIZ_INTERVAL} steps, Downscale: {REAL_TIME_VIZ_DOWNSCALE}, Type: '{REAL_TIME_VIZ_TYPE}').\")\n",
    "        else:\n",
    "             print(\"‚ÑπÔ∏è Real-time visualization is disabled.\")\n",
    "        if LARGE_SIM_CHECKPOINT_INTERVAL is not None and LARGE_SIM_CHECKPOINT_INTERVAL > 0:\n",
    "             print(f\"üíæ Raw state checkpoints will be saved every {LARGE_SIM_CHECKPOINT_INTERVAL} steps in '{LARGE_SIM_CHECKPOINT_DIR}'.\")\n",
    "        else:\n",
    "             print(\"‚ÑπÔ∏è Raw state checkpointing is disabled.\")\n",
    "\n",
    "        writer_density, writer_channels, writer_magnitude, writer_phase, writer_change = None, None, None, None, None\n",
    "        if VIDEO_SAVE_INTERVAL_STEPS is not None and VIDEO_SAVE_INTERVAL_STEPS > 0:\n",
    "            try:\n",
    "                writer_density = imageio.get_writer(MP4_DENSITY_FILENAME, fps=VIDEO_FPS, codec='libx264', quality=VIDEO_QUALITY)\n",
    "                writer_channels = imageio.get_writer(MP4_CHANNELS_FILENAME, fps=VIDEO_FPS, codec='libx264', quality=VIDEO_QUALITY)\n",
    "                writer_magnitude = imageio.get_writer(MP4_MAGNITUDE_FILENAME, fps=VIDEO_FPS, codec='libx264', quality=VIDEO_QUALITY)\n",
    "                writer_phase = imageio.get_writer(MP4_PHASE_FILENAME, fps=VIDEO_FPS, codec='libx264', quality=VIDEO_QUALITY)\n",
    "                writer_change = imageio.get_writer(MP4_CHANGE_FILENAME, fps=VIDEO_FPS, codec='libx264', quality=VIDEO_QUALITY)\n",
    "                print(\"‚úÖ Video writers initialized.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error initializing video writers: {e}\")\n",
    "                writer_density, writer_channels, writer_magnitude, writer_phase, writer_change = None, None, None, None, None\n",
    "\n",
    "        prev_state_for_change_viz = None\n",
    "        if start_step > 0:\n",
    "            prev_step_checkpoint_filepath = os.path.join(LARGE_SIM_CHECKPOINT_DIR, f\"large_sim_state_step_{start_step-1}.pth\")\n",
    "            if os.path.exists(prev_step_checkpoint_filepath):\n",
    "                print(f\"Attempting to load previous state for change visualization from: {prev_step_checkpoint_filepath}\")\n",
    "                temp_operator = QCA_Operator_Deep(D_STATE, HIDDEN_CHANNELS).to(DEVICE)\n",
    "                temp_motor = Aetheria_Motor(GRID_SIZE_INFERENCE, D_STATE, temp_operator)\n",
    "                if load_qca_state(temp_motor, prev_step_checkpoint_filepath) != -1:\n",
    "                    prev_state_for_change_viz = temp_motor.state\n",
    "                    print(\"Loaded previous state for change visualization.\")\n",
    "                else:\n",
    "                    print(\"‚ùå Failed to load previous state for change visualization.\")\n",
    "            else:\n",
    "                print(f\"‚ùå Previous state checkpoint for step {start_step-1} not found. Change video/viz might be inaccurate.\")\n",
    "\n",
    "        if prev_state_for_change_viz is None and ((VIDEO_SAVE_INTERVAL_STEPS is not None and VIDEO_SAVE_INTERVAL_STEPS > 0) or \\\n",
    "           (REAL_TIME_VIZ_INTERVAL is not None and REAL_TIME_VIZ_INTERVAL > 0 and REAL_TIME_VIZ_TYPE == 'change')):\n",
    "            prev_state_for_change_viz = QCA_State(large_scale_motor.state.size, large_scale_motor.state.d_state)\n",
    "            prev_state_for_change_viz.x_real.data = large_scale_motor.state.x_real.data.clone().to(DEVICE)\n",
    "            prev_state_for_change_viz.x_imag.data = large_scale_motor.state.x_imag.data.clone().to(DEVICE)\n",
    "            print(\"Initialized previous state clone for change visualization.\")\n",
    "\n",
    "        t = start_step\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                for t in range(start_step, NUM_INFERENCE_STEPS):\n",
    "\n",
    "                    current_state_clone_for_change_viz = None\n",
    "                    if prev_state_for_change_viz is not None:\n",
    "                        current_state_clone_for_change_viz = QCA_State(large_scale_motor.state.size, large_scale_motor.state.d_state)\n",
    "                        current_state_clone_for_change_viz.x_real.data = large_scale_motor.state.x_real.data.clone().to(DEVICE)\n",
    "                        current_state_clone_for_change_viz.x_imag.data = large_scale_motor.state.x_imag.data.clone().to(DEVICE)\n",
    "\n",
    "                    large_scale_motor.evolve_step()\n",
    "                    current_state = large_scale_motor.state\n",
    "\n",
    "                    # Real-time Visualization (Print statement only)\n",
    "                    if REAL_TIME_VIZ_INTERVAL is not None and REAL_TIME_VIZ_INTERVAL > 0 and (t + 1) % REAL_TIME_VIZ_INTERVAL == 0:\n",
    "                        print(f\"--- Real-time frame {t+1} (display disabled in .py script) ---\")\n",
    "                        # (Original display(Image.fromarray(...)) call removed)\n",
    "\n",
    "                    # Save Video Frame\n",
    "                    if VIDEO_SAVE_INTERVAL_STEPS is not None and VIDEO_SAVE_INTERVAL_STEPS > 0 and ((t + 1) % VIDEO_SAVE_INTERVAL_STEPS == 0 or (t == start_step and start_step == 0)):\n",
    "                        try:\n",
    "                            density_frame = get_density_frame_gpu(current_state)\n",
    "                            channels_frame = get_channel_frame_gpu(current_state, num_channels=min(3, D_STATE))\n",
    "                            magnitude_frame = get_state_magnitude_frame_gpu(current_state)\n",
    "                            phase_frame = get_state_phase_frame_gpu(current_state)\n",
    "\n",
    "                            if VIDEO_DOWNSCALE_FACTOR > 1:\n",
    "                                density_frame = downscale_frame(density_frame, VIDEO_DOWNSCALE_FACTOR)\n",
    "                                channels_frame = downscale_frame(channels_frame, VIDEO_DOWNSCALE_FACTOR)\n",
    "                                magnitude_frame = downscale_frame(magnitude_frame, VIDEO_DOWNSCALE_FACTOR)\n",
    "                                phase_frame = downscale_frame(phase_frame, VIDEO_DOWNSCALE_FACTOR)\n",
    "\n",
    "                            if writer_density: writer_density.append_data(density_frame)\n",
    "                            if writer_channels: writer_channels.append_data(channels_frame)\n",
    "                            if writer_magnitude: writer_magnitude.append_data(magnitude_frame)\n",
    "                            if writer_phase: writer_phase.append_data(phase_frame)\n",
    "\n",
    "                            if prev_state_for_change_viz is not None:\n",
    "                                change_frame = get_state_change_magnitude_frame_gpu(current_state, prev_state_for_change_viz)\n",
    "                                if VIDEO_DOWNSCALE_FACTOR > 1:\n",
    "                                     change_frame = downscale_frame(change_frame, VIDEO_DOWNSCALE_FACTOR)\n",
    "                                if writer_change: writer_change.append_data(change_frame)\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ö†Ô∏è  Error generating/saving video frame at step {t+1}: {e}\")\n",
    "\n",
    "                    if current_state_clone_for_change_viz is not None:\n",
    "                        prev_state_for_change_viz = current_state_clone_for_change_viz\n",
    "\n",
    "                    # Save State Checkpoint\n",
    "                    if LARGE_SIM_CHECKPOINT_INTERVAL is not None and LARGE_SIM_CHECKPOINT_INTERVAL > 0 and (t + 1) % LARGE_SIM_CHECKPOINT_INTERVAL == 0:\n",
    "                        save_qca_state(large_scale_motor, t + 1, LARGE_SIM_CHECKPOINT_DIR)\n",
    "\n",
    "                    # Print Progress\n",
    "                    if (t + 1) % max(1, (NUM_INFERENCE_STEPS // 20)) == 0 or (t + 1) == NUM_INFERENCE_STEPS:\n",
    "                        print(f\"üìà Simulation Progress: {t+1}/{NUM_INFERENCE_STEPS} steps completed.\")\n",
    "\n",
    "                    if (t + 1) % 200 == 0:\n",
    "                        gc.collect()\n",
    "                        if torch.cuda.is_available():\n",
    "                            torch.cuda.empty_cache()\n",
    "\n",
    "                # Save final checkpoint\n",
    "                if LARGE_SIM_CHECKPOINT_INTERVAL is not None and LARGE_SIM_CHECKPOINT_INTERVAL > 0 and (NUM_INFERENCE_STEPS % LARGE_SIM_CHECKPOINT_INTERVAL != 0):\n",
    "                     save_qca_state(large_scale_motor, NUM_INFERENCE_STEPS, LARGE_SIM_CHECKPOINT_DIR)\n",
    "                print(\"‚úÖ Simulation loop completed.\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(f\"\\n‚èπÔ∏è Simulation interrupted by user at step {t+1}.\")\n",
    "            if LARGE_SIM_CHECKPOINT_INTERVAL is not None and LARGE_SIM_CHECKPOINT_INTERVAL > 0:\n",
    "                save_qca_state(large_scale_motor, t + 1, LARGE_SIM_CHECKPOINT_DIR)\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ùå Error during simulation at step {t+1}: {e}\")\n",
    "            if LARGE_SIM_CHECKPOINT_INTERVAL is not None and LARGE_SIM_CHECKPOINT_INTERVAL > 0:\n",
    "                save_qca_state(large_scale_motor, t + 1, LARGE_SIM_CHECKPOINT_DIR)\n",
    "            raise e # Re-raise error\n",
    "        finally:\n",
    "            try:\n",
    "                if writer_density: writer_density.close()\n",
    "                if writer_channels: writer_channels.close()\n",
    "                if writer_magnitude: writer_magnitude.close()\n",
    "                if writer_phase: writer_phase.close()\n",
    "                if writer_change: writer_change.close()\n",
    "                if any([writer_density, writer_channels, writer_magnitude, writer_phase, writer_change]):\n",
    "                     print(\"üìπ Video writers closed successfully.\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error closing video writers: {e}\")\n",
    "\n",
    "        # --- Final Summary ---\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üéâ PROLONGED LARGE SIMULATION FINISHED (Completed or Interrupted)\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"üìÅ Video files generated in this run (if video saving was enabled):\")\n",
    "        if VIDEO_SAVE_INTERVAL_STEPS is not None and VIDEO_SAVE_INTERVAL_STEPS > 0:\n",
    "            print(f\"   ‚Ä¢ Density: {MP4_DENSITY_FILENAME}\")\n",
    "            print(f\"   ‚Ä¢ Channels: {MP4_CHANNELS_FILENAME}\")\n",
    "            print(f\"   ‚Ä¢ Magnitude: {MP4_MAGNITUDE_FILENAME}\")\n",
    "            print(f\"   ‚Ä¢ Phase: {MP4_PHASE_FILENAME}\")\n",
    "            print(f\"   ‚Ä¢ Change: {MP4_CHANGE_FILENAME}\")\n",
    "        else:\n",
    "            print(\"   ‚Ä¢ Video saving was disabled.\")\n",
    "        print(f\"   ‚Ä¢ Raw state checkpoints in: {LARGE_SIM_CHECKPOINT_DIR}/\")\n",
    "\n",
    "        if VIDEO_SAVE_INTERVAL_STEPS is not None and VIDEO_SAVE_INTERVAL_STEPS > 0 and os.path.exists(MP4_DENSITY_FILENAME):\n",
    "            file_size = os.path.getsize(MP4_DENSITY_FILENAME) / (1024*1024)\n",
    "            print(f\"üìä Estimated size of Density video: {file_size:.1f} MB\")\n",
    "\n",
    "        print(\"\\nüéØ Download Instructions:\")\n",
    "        print(\"   You can find the saved MP4 files and .pth checkpoints in your working directory\")\n",
    "        print(f\"   (or '{LARGE_SIM_CHECKPOINT_DIR}/' for state checkpoints).\")\n",
    "\n",
    "        print(\"\\n>>> PROLONGED LARGE SIMULATION PHASE (PHASE 7) COMPLETED <<<\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\n>>> PROLONGED LARGE SIMULATION PHASE (PHASE 7) SKIPPED <<<\")\n",
    "\n",
    "    print(\"\\n--- AETHERIA PIPELINE EXECUTION FINISHED ---\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SCRIPT ENTRY POINT\n",
    "# ==============================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    main_pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

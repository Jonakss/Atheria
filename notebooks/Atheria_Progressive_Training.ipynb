{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83d\ude80 Atheria 4: Entrenamiento Progresivo (Long-Running)\n",
        "\n",
        "Notebook optimizado para **entrenamientos largos** (muchas horas) con aprovechamiento m\u00e1ximo de cuota de GPU.\n",
        "\n",
        "## \u2728 Caracter\u00edsticas\n",
        "\n",
        "- \ud83d\udd04 **Auto-guardado en Google Drive** - Sincronizaci\u00f3n configurable\n",
        "- \ud83d\udcca **Monitoreo de recursos** - GPU%, RAM, tiempo de sesi\u00f3n\n",
        "- \u26a1 **Auto-recuperaci\u00f3n** - Contin\u00faa autom\u00e1ticamente desde checkpoints\n",
        "- \u23f0 **L\u00edmite de tiempo autom\u00e1tico** - Guardado de emergencia antes de timeout\n",
        "- \ud83d\udcc8 **Visualizaci\u00f3n en tiempo real** - Progreso, m\u00e9tricas, recursos\n",
        "- \ud83d\udcbe **Smart Checkpointing** - Solo guarda mejores modelos + \u00faltimo\n",
        "\n",
        "## \ud83d\udccb Cuotas de GPU\n",
        "\n",
        "- **Colab Free**: ~12 horas/d\u00eda (variable)\n",
        "- **Colab Pro**: ~24 horas continuas\n",
        "- **Kaggle**: 30 horas/semana (T4/P100)\n",
        "\n",
        "## \ud83c\udfaf Workflow Recomendado\n",
        "\n",
        "1. Configurar experimento (Secci\u00f3n 4)\n",
        "2. Ejecutar todas las celdas autom\u00e1ticamente\n",
        "3. Dejar corriendo sin supervisi\u00f3n\n",
        "4. Checkpoints se guardan autom\u00e1ticamente en Drive\n",
        "5. Si se desconecta: ejecutar de nuevo, auto-recupera desde Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udce6 Secci\u00f3n 1: Setup y Detecci\u00f3n de Entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detectar entorno (Kaggle o Colab)\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# IMPORTANTE: Verificar Kaggle PRIMERO (tiene google.colab instalado pero no funciona)\n",
        "IN_KAGGLE = os.path.exists(\"/kaggle/input\") or os.path.exists(\"/kaggle/working\")\n",
        "\n",
        "# Solo si NO es Kaggle, verificar Colab\n",
        "if not IN_KAGGLE:\n",
        "    try:\n",
        "        import google.colab\n",
        "        # Verificar que realmente estemos en Colab (existe /content)\n",
        "        IN_COLAB = os.path.exists(\"/content\")\n",
        "    except:\n",
        "        IN_COLAB = False\n",
        "else:\n",
        "    IN_COLAB = False\n",
        "\n",
        "ENV_NAME = \"Kaggle\" if IN_KAGGLE else \"Colab\" if IN_COLAB else \"Local\"\n",
        "print(f\"\ud83c\udf0d Entorno detectado: {ENV_NAME}\")\n",
        "\n",
        "# Instalar dependencias b\u00e1sicas\n",
        "print(\"\ud83d\udce6 Instalando dependencias...\")\n",
        "%pip install -q snntorch scikit-learn matplotlib\n",
        "\n",
        "# Para Colab: instalar pybind11 (opcional, solo para motor nativo)\n",
        "if IN_COLAB:\n",
        "    %pip install -q pybind11\n",
        "\n",
        "print(\"\u2705 Dependencias instaladas\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udcbe Secci\u00f3n 2: Google Drive - Montaje y Configuraci\u00f3n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Montar Google Drive (solo Colab)\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    print(\"\ud83d\udcc1 Montando Google Drive...\")\n",
        "    drive.mount('/content/drive')\n",
        "    DRIVE_ROOT = Path(\"/content/drive/MyDrive/Atheria\")\n",
        "    print(f\"\u2705 Drive montado en: {DRIVE_ROOT}\")\n",
        "elif IN_KAGGLE:\n",
        "    # En Kaggle, usar /kaggle/working como alternativa\n",
        "    DRIVE_ROOT = Path(\"/kaggle/working/atheria_checkpoints\")\n",
        "    print(f\"\ud83d\udcc1 Usando directorio local: {DRIVE_ROOT}\")\n",
        "else:\n",
        "    DRIVE_ROOT = Path.home() / \"atheria_checkpoints\"\n",
        "    print(f\"\ud83d\udcbb Usando directorio local: {DRIVE_ROOT}\")\n",
        "\n",
        "# Crear estructura de carpetas\n",
        "DRIVE_CHECKPOINT_DIR = DRIVE_ROOT / \"checkpoints\"\n",
        "DRIVE_LOGS_DIR = DRIVE_ROOT / \"logs\"\n",
        "DRIVE_EXPORTS_DIR = DRIVE_ROOT / \"exports\"\n",
        "\n",
        "for directory in [DRIVE_CHECKPOINT_DIR, DRIVE_LOGS_DIR, DRIVE_EXPORTS_DIR]:\n",
        "    directory.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"\\n\ud83d\udcc2 Estructura de carpetas creada:\")\n",
        "print(f\"  - Checkpoints: {DRIVE_CHECKPOINT_DIR}\")\n",
        "print(f\"  - Logs: {DRIVE_LOGS_DIR}\")\n",
        "print(f\"  - Exports: {DRIVE_EXPORTS_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udd27 Secci\u00f3n 3: Clonar Proyecto Atheria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configurar ruta del proyecto\n",
        "if IN_KAGGLE:\n",
        "    PROJECT_ROOT = Path(\"/kaggle/working/Atheria\")\n",
        "    if not PROJECT_ROOT.exists():\n",
        "        print(\"\u26a0\ufe0f Proyecto no encontrado. Clonando desde GitHub...\")\n",
        "        !git clone https://github.com/Jonakss/Atheria.git /kaggle/working/Atheria\n",
        "elif IN_COLAB:\n",
        "    PROJECT_ROOT = Path(\"/content/Atheria\")\n",
        "    if not PROJECT_ROOT.exists():\n",
        "        print(\"\ud83d\udce5 Clonando proyecto desde GitHub...\")\n",
        "        !git clone https://github.com/Jonakss/Atheria.git /content/Atheria\n",
        "else:\n",
        "    # Local: asumir que estamos en notebooks/\n",
        "    PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
        "\n",
        "# Agregar al path de Python\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "print(f\"\ud83d\udcc1 Proyecto configurado en: {PROJECT_ROOT}\")\n",
        "\n",
        "# Verificar estructura b\u00e1sica\n",
        "src_path = PROJECT_ROOT / \"src\"\n",
        "if src_path.exists():\n",
        "    print(\"\u2705 Estructura del proyecto verificada\")\n",
        "else:\n",
        "    print(\"\u274c Error: No se encontr\u00f3 la carpeta 'src'. Verifica la instalaci\u00f3n.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udcca Secci\u00f3n 4: Utilidades de Monitoreo de Recursos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import psutil\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "from IPython.display import clear_output\n",
        "\n",
        "class ResourceMonitor:\n",
        "    \"\"\"Monitor de recursos GPU/RAM/Tiempo\"\"\"\n",
        "    \n",
        "    def __init__(self, max_training_hours=10):\n",
        "        self.start_time = time.time()\n",
        "        self.max_training_seconds = max_training_hours * 3600\n",
        "        self.max_training_hours = max_training_hours\n",
        "        \n",
        "    def get_gpu_usage(self):\n",
        "        \"\"\"Retorna uso de GPU en %\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return 0.0\n",
        "        try:\n",
        "            return torch.cuda.utilization()\n",
        "        except:\n",
        "            return 0.0\n",
        "    \n",
        "    def get_gpu_memory(self):\n",
        "        \"\"\"Retorna memoria GPU usada/total en GB\"\"\"\n",
        "        if not torch.cuda.is_available():\n",
        "            return 0.0, 0.0\n",
        "        allocated = torch.cuda.memory_allocated() / 1e9  # GB\n",
        "        reserved = torch.cuda.memory_reserved() / 1e9\n",
        "        return allocated, reserved\n",
        "    \n",
        "    def get_ram_usage(self):\n",
        "        \"\"\"Retorna uso de RAM en GB\"\"\"\n",
        "        mem = psutil.virtual_memory()\n",
        "        return mem.used / 1e9, mem.total / 1e9\n",
        "    \n",
        "    def get_elapsed_time(self):\n",
        "        \"\"\"Retorna tiempo transcurrido y restante\"\"\"\n",
        "        elapsed = time.time() - self.start_time\n",
        "        remaining = max(0, self.max_training_seconds - elapsed)\n",
        "        return elapsed, remaining\n",
        "    \n",
        "    def should_stop(self):\n",
        "        \"\"\"True si se acerca al l\u00edmite de tiempo (90%)\"\"\"\n",
        "        elapsed, remaining = self.get_elapsed_time()\n",
        "        return remaining < (self.max_training_seconds * 0.1)  # 10% restante\n",
        "    \n",
        "    def get_status_str(self):\n",
        "        \"\"\"Retorna string con estado de recursos\"\"\"\n",
        "        gpu_usage = self.get_gpu_usage()\n",
        "        gpu_mem_used, gpu_mem_reserved = self.get_gpu_memory()\n",
        "        ram_used, ram_total = self.get_ram_usage()\n",
        "        elapsed, remaining = self.get_elapsed_time()\n",
        "        \n",
        "        elapsed_str = str(timedelta(seconds=int(elapsed)))\n",
        "        remaining_str = str(timedelta(seconds=int(remaining)))\n",
        "        \n",
        "        status = (\n",
        "            f\"\ud83d\udcca RECURSOS:\\n\"\n",
        "            f\"  GPU Utilization: {gpu_usage:.1f}%\\n\"\n",
        "            f\"  GPU Memory: {gpu_mem_used:.2f}GB / {gpu_mem_reserved:.2f}GB\\n\"\n",
        "            f\"  RAM: {ram_used:.2f}GB / {ram_total:.2f}GB ({ram_used/ram_total*100:.1f}%)\\n\"\n",
        "            f\"  \\n\"\n",
        "            f\"\u23f0 TIEMPO:\\n\"\n",
        "            f\"  Transcurrido: {elapsed_str}\\n\"\n",
        "            f\"  Restante: {remaining_str} (de {self.max_training_hours}h m\u00e1ximo)\\n\"\n",
        "        )\n",
        "        return status\n",
        "\n",
        "print(\"\u2705 ResourceMonitor definido\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \u2699\ufe0f Secci\u00f3n 5: Configuraci\u00f3n del Experimento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from types import SimpleNamespace\n",
        "import torch\n",
        "\n",
        "# ============================================================================\n",
        "# \ud83c\udfaf CONFIGURACI\u00d3N DEL CURRICULUM (FASES DE ENTRENAMIENTO)\n",
        "# ============================================================================\n",
        "\n",
        "# Definimos una lista de fases. El sistema ejecutar\u00e1 una tras otra.\n",
        "# Si una fase ya est\u00e1 completada (existe checkpoint final), pasar\u00e1 a la siguiente.\n",
        "\n",
        "TRAINING_PHASES = [\n",
        "    # --- FASE 1: ESTABILIDAD DEL VAC\u00cdO ---\n",
        "    {\n",
        "        \"PHASE_NAME\": \"Fase1_Vacuum_Stability\",\n",
        "        \"LOAD_FROM_PHASE\": None,  # Empezar desde cero\n",
        "        \n",
        "        \"MODEL_ARCHITECTURE\": \"UNET\",\n",
        "        \"MODEL_PARAMS\": {\n",
        "            \"d_state\": 4,           # Dimensi\u00f3n baja para aprender r\u00e1pido\n",
        "            \"hidden_channels\": 32,\n",
        "        },\n",
        "        \n",
        "        \"GRID_SIZE_TRAINING\": 32,\n",
        "        \"QCA_STEPS_TRAINING\": 50,\n",
        "        \"LR_RATE_M\": 1e-4,\n",
        "        \"GAMMA_DECAY\": 0.001,       # Poco decaimiento\n",
        "        \n",
        "        \"TOTAL_EPISODES\": 500,\n",
        "        \"SAVE_EVERY_EPISODES\": 50,\n",
        "    },\n",
        "    \n",
        "    # --- FASE 2: EMERGENCIA DE MATERIA ---\n",
        "    {\n",
        "        \"PHASE_NAME\": \"Fase2_Matter_Emergence\",\n",
        "        \"LOAD_FROM_PHASE\": \"Fase1_Vacuum_Stability\", # Cargar cerebro de Fase 1\n",
        "        \n",
        "        \"MODEL_ARCHITECTURE\": \"UNET\",\n",
        "        \"MODEL_PARAMS\": {\n",
        "            \"d_state\": 4,           # Misma dimensi\u00f3n\n",
        "            \"hidden_channels\": 64,  # M\u00e1s capacidad (neuronas)\n",
        "        },\n",
        "        \n",
        "        \"GRID_SIZE_TRAINING\": 64,   # Grid m\u00e1s grande\n",
        "        \"QCA_STEPS_TRAINING\": 100,\n",
        "        \"LR_RATE_M\": 5e-5,          # LR m\u00e1s fino\n",
        "        \"GAMMA_DECAY\": 0.01,        # M\u00e1s presi\u00f3n evolutiva\n",
        "        \n",
        "        \"TOTAL_EPISODES\": 1000,\n",
        "        \"SAVE_EVERY_EPISODES\": 20,\n",
        "    },\n",
        "]\n",
        "\n",
        "# Configuraci\u00f3n Global\n",
        "GLOBAL_CONFIG = {\n",
        "    \"DRIVE_SYNC_EVERY\": 50,\n",
        "    \"MAX_TRAINING_HOURS\": 10,     # Tiempo total para TODAS las fases\n",
        "    \"AUTO_RESUME\": True,\n",
        "    \"MAX_CHECKPOINTS_TO_KEEP\": 3,\n",
        "}\n",
        "\n",
        "# Detectar dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Directorios base\n",
        "EXPERIMENT_ROOT_NAME = \"MultiPhase_Experiment_v1\" # Nombre carpeta ra\u00edz\n",
        "BASE_CHECKPOINT_DIR = DRIVE_CHECKPOINT_DIR / EXPERIMENT_ROOT_NAME\n",
        "BASE_LOG_DIR = DRIVE_LOGS_DIR / EXPERIMENT_ROOT_NAME\n",
        "BASE_CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BASE_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"\ud83d\udcca CONFIGURACI\u00d3N DEL CURRICULUM\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Experimento Ra\u00edz: {EXPERIMENT_ROOT_NAME}\")\n",
        "print(f\"Total Fases: {len(TRAINING_PHASES)}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, phase in enumerate(TRAINING_PHASES):\n",
        "    print(f\"\\n\ud83d\udd39 FASE {i+1}: {phase['PHASE_NAME']}\")\n",
        "    print(f\"   Load From: {phase['LOAD_FROM_PHASE']}\")\n",
        "    print(f\"   Model: {phase['MODEL_ARCHITECTURE']} (d={phase['MODEL_PARAMS']['d_state']}, ch={phase['MODEL_PARAMS']['hidden_channels']})\")\n",
        "    print(f\"   Grid: {phase['GRID_SIZE_TRAINING']}x{phase['GRID_SIZE_TRAINING']}\")\n",
        "    print(f\"   Episodes: {phase['TOTAL_EPISODES']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udd04 Secci\u00f3n 6: Verificar/Cargar Checkpoint Existente (Auto-Resume)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def find_latest_checkpoint(checkpoint_dir):\n",
        "    \"\"\"Encuentra el \u00faltimo checkpoint en un directorio\"\"\"\n",
        "    checkpoints = list(checkpoint_dir.glob(\"*.pth\"))\n",
        "    if not checkpoints:\n",
        "        return None\n",
        "    \n",
        "    # Buscar last_model.pth primero\n",
        "    last_model = checkpoint_dir / \"last_model.pth\"\n",
        "    if last_model.exists():\n",
        "        return str(last_model)\n",
        "    \n",
        "    # Si no, buscar el m\u00e1s reciente\n",
        "    latest = max(checkpoints, key=lambda p: p.stat().st_mtime)\n",
        "    return str(latest)\n",
        "\n",
        "# Buscar checkpoint existente\n",
        "checkpoint_path = None\n",
        "resume_from_episode = 0\n",
        "\n",
        "if exp_cfg.AUTO_RESUME:\n",
        "    print(\"\ud83d\udd0d Buscando checkpoint existente...\")\n",
        "    \n",
        "    # Primero buscar en Drive\n",
        "    drive_checkpoint = find_latest_checkpoint(EXPERIMENT_CHECKPOINT_DIR)\n",
        "    if drive_checkpoint:\n",
        "        checkpoint_path = drive_checkpoint\n",
        "        print(f\"\u2705 Checkpoint encontrado en Drive: {Path(checkpoint_path).name}\")\n",
        "    else:\n",
        "        # Si no hay en Drive, buscar en local\n",
        "        local_checkpoint = find_latest_checkpoint(LOCAL_CHECKPOINT_DIR)\n",
        "        if local_checkpoint:\n",
        "            checkpoint_path = local_checkpoint\n",
        "            print(f\"\u2705 Checkpoint encontrado localmente: {Path(checkpoint_path).name}\")\n",
        "    \n",
        "    if checkpoint_path:\n",
        "        # Cargar checkpoint para ver en qu\u00e9 episodio estamos\n",
        "        try:\n",
        "            ckpt_data = torch.load(checkpoint_path, map_location='cpu')\n",
        "            resume_from_episode = ckpt_data.get('episode', 0)\n",
        "            print(f\"\ud83d\udd04 Continuando desde episodio {resume_from_episode}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"\u26a0\ufe0f Error leyendo checkpoint: {e}\")\n",
        "            checkpoint_path = None\n",
        "\n",
        "if checkpoint_path is None:\n",
        "    print(\"\ud83c\udd95 No se encontr\u00f3 checkpoint previo. Iniciando desde cero.\")\n",
        "\n",
        "# Calcular episodios restantes\n",
        "episodes_remaining = max(0, exp_cfg.TOTAL_EPISODES - resume_from_episode)\n",
        "print(f\"\\n\ud83d\udcc8 Episodios restantes: {episodes_remaining}/{exp_cfg.TOTAL_EPISODES}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "from src.trainers.qc_trainer_v4 import QC_Trainer_v4\n",
        "from src.model_loader import instantiate_model, load_weights, load_checkpoint_data\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output, display\n",
        "import json\n",
        "import shutil\n",
        "\n",
        "# Inicializar monitor de recursos GLOBAL\n",
        "monitor = ResourceMonitor(max_training_hours=GLOBAL_CONFIG[\"MAX_TRAINING_HOURS\"])\n",
        "\n",
        "# Funci\u00f3n auxiliar para guardar en Drive\n",
        "def sync_checkpoint_to_drive(local_path, drive_dir, filename=None):\n",
        "    \"\"\"Copia checkpoint local a Drive\"\"\"\n",
        "    try:\n",
        "        if filename is None:\n",
        "            filename = Path(local_path).name\n",
        "        drive_path = drive_dir / filename\n",
        "        shutil.copy2(local_path, drive_path)\n",
        "        # logger.info(f\"\ud83d\udcbe Checkpoint sincronizado a Drive: {filename}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\u274c Error sincronizando a Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "# ============================================================================\n",
        "# \ud83d\udd04 LOOP PRINCIPAL DE FASES\n",
        "# ============================================================================\n",
        "\n",
        "for phase_idx, phase_cfg in enumerate(TRAINING_PHASES):\n",
        "    PHASE_NAME = phase_cfg[\"PHASE_NAME\"]\n",
        "    \n",
        "    print(\"\\n\" + \"#\" * 70)\n",
        "    print(f\"\ud83d\ude80 INICIANDO FASE {phase_idx+1}/{len(TRAINING_PHASES)}: {PHASE_NAME}\")\n",
        "    print(\"#\" * 70)\n",
        "    \n",
        "    # 1. Configuraci\u00f3n de Directorios para esta Fase\n",
        "    PHASE_CHECKPOINT_DIR = BASE_CHECKPOINT_DIR / PHASE_NAME\n",
        "    PHASE_LOG_DIR = BASE_LOG_DIR / PHASE_NAME\n",
        "    PHASE_CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    PHASE_LOG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    LOCAL_PHASE_DIR = PROJECT_ROOT / \"output\" / \"checkpoints\" / EXPERIMENT_ROOT_NAME / PHASE_NAME\n",
        "    LOCAL_PHASE_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    # 2. Verificar si la fase ya est\u00e1 completada\n",
        "    final_marker = PHASE_CHECKPOINT_DIR / \"PHASE_COMPLETED.marker\"\n",
        "    if final_marker.exists() and GLOBAL_CONFIG[\"AUTO_RESUME\"]:\n",
        "        print(f\"\u2705 Fase {PHASE_NAME} ya completada. Saltando...\")\n",
        "        continue\n",
        "\n",
        "    # 3. Preparar Configuraci\u00f3n de Fase\n",
        "    current_exp_cfg = SimpleNamespace(**phase_cfg)\n",
        "    current_exp_cfg.MODEL_PARAMS = SimpleNamespace(**phase_cfg[\"MODEL_PARAMS\"])\n",
        "    current_exp_cfg.DEVICE = device\n",
        "    \n",
        "    # 4. Instanciar Modelo\n",
        "    print(f\"\ud83d\udee0\ufe0f Instanciando modelo {phase_cfg['MODEL_ARCHITECTURE']}...\")\n",
        "    model = instantiate_model(current_exp_cfg)\n",
        "    \n",
        "    # 5. Cargar Pesos (L\u00f3gica de Transici\u00f3n)\n",
        "    resume_from_episode = 0\n",
        "    weights_loaded = False\n",
        "    \n",
        "    # A) Intentar resumir la propia fase (si se interrumpi\u00f3)\n",
        "    if GLOBAL_CONFIG[\"AUTO_RESUME\"]:\n",
        "        latest_ckpt = find_latest_checkpoint(PHASE_CHECKPOINT_DIR)\n",
        "        if not latest_ckpt:\n",
        "            latest_ckpt = find_latest_checkpoint(LOCAL_PHASE_DIR)\n",
        "            \n",
        "        if latest_ckpt:\n",
        "            print(f\"\ud83d\udd04 Resumiendo fase actual desde: {Path(latest_ckpt).name}\")\n",
        "            ckpt_data = load_checkpoint_data(latest_ckpt)\n",
        "            if ckpt_data:\n",
        "                load_weights(model, ckpt_data) # Strict=True porque es el mismo modelo\n",
        "                resume_from_episode = ckpt_data.get('episode', 0)\n",
        "                weights_loaded = True\n",
        "    \n",
        "    # B) Si no se resume, cargar de fase anterior (Transfer Learning)\n",
        "    if not weights_loaded and phase_cfg[\"LOAD_FROM_PHASE\"]:\n",
        "        prev_phase_name = phase_cfg[\"LOAD_FROM_PHASE\"]\n",
        "        print(f\"\ud83d\udce5 Buscando pesos de fase anterior: {prev_phase_name}\")\n",
        "        \n",
        "        prev_dir = BASE_CHECKPOINT_DIR / prev_phase_name\n",
        "        best_prev = prev_dir / \"best_model.pth\"\n",
        "        if not best_prev.exists():\n",
        "             best_prev = find_latest_checkpoint(prev_dir)\n",
        "             \n",
        "        if best_prev and Path(best_prev).exists():\n",
        "            print(f\"\u2705 Cargando pesos previos de: {Path(best_prev).name}\")\n",
        "            ckpt_data = load_checkpoint_data(str(best_prev))\n",
        "            \n",
        "            # CR\u00cdTICO: strict=False para permitir cambio de arquitectura (ej: d_state 4 -> 8)\n",
        "            missing, unexpected = model.load_state_dict(ckpt_data['model_state_dict'], strict=False)\n",
        "            print(f\"\u2139\ufe0f Transfer Learning: {len(missing)} capas nuevas inicializadas, {len(unexpected)} capas descartadas.\")\n",
        "            weights_loaded = True\n",
        "        else:\n",
        "            print(f\"\u26a0\ufe0f No se encontraron pesos de la fase anterior. Iniciando desde cero (aleatorio).\")\n",
        "            \n",
        "    if not weights_loaded:\n",
        "        print(\"\ud83c\udd95 Iniciando fase con pesos aleatorios.\")\n",
        "\n",
        "    # 6. Inicializar Trainer\n",
        "    trainer = QC_Trainer_v4(\n",
        "        experiment_name=f\"{EXPERIMENT_ROOT_NAME}/{PHASE_NAME}\", # Subdirectorio en logs internos\n",
        "        model=model,\n",
        "        device=device,\n",
        "        lr=phase_cfg[\"LR_RATE_M\"],\n",
        "        grid_size=phase_cfg[\"GRID_SIZE_TRAINING\"],\n",
        "        qca_steps=phase_cfg[\"QCA_STEPS_TRAINING\"],\n",
        "        gamma_decay=phase_cfg[\"GAMMA_DECAY\"],\n",
        "        max_checkpoints_to_keep=GLOBAL_CONFIG[\"MAX_CHECKPOINTS_TO_KEEP\"]\n",
        "    )\n",
        "    \n",
        "    # Hack para redirigir checkpoints del trainer a nuestra carpeta de fase local\n",
        "    trainer.checkpoint_dir = str(LOCAL_PHASE_DIR)\n",
        "    \n",
        "    # Tracking local\n",
        "    training_log = {\"episodes\": [], \"losses\": []}\n",
        "    \n",
        "    # 7. Loop de Episodios\n",
        "    print(f\"\\n\u25b6\ufe0f Entrenando {phase_cfg['TOTAL_EPISODES'] - resume_from_episode} episodios...\")\n",
        "    \n",
        "    try:\n",
        "        for episode in range(resume_from_episode, phase_cfg[\"TOTAL_EPISODES\"]):\n",
        "            \n",
        "            # Check tiempo global\n",
        "            if monitor.should_stop():\n",
        "                print(f\"\\n\u23f0 TIEMPO GLOBAL AGOTADO en Fase {PHASE_NAME}, Ep {episode}\")\n",
        "                trainer.save_checkpoint(episode, is_best=False)\n",
        "                sync_checkpoint_to_drive(str(LOCAL_PHASE_DIR / f\"checkpoint_ep{episode}.pth\"), PHASE_CHECKPOINT_DIR)\n",
        "                raise KeyboardInterrupt(\"Global Timeout\") # Salir de todo\n",
        "                \n",
        "            # Entrenar\n",
        "            epoch_result = trainer.train_episode(episode)\n",
        "            loss = epoch_result.get(\"loss_total\", 0)\n",
        "            \n",
        "            training_log[\"episodes\"].append(episode)\n",
        "            training_log[\"losses\"].append(loss)\n",
        "            \n",
        "            # Guardar/Sync\n",
        "            if (episode + 1) % phase_cfg[\"SAVE_EVERY_EPISODES\"] == 0:\n",
        "                is_best = (episode + 1) % (phase_cfg[\"SAVE_EVERY_EPISODES\"] * 2) == 0\n",
        "                trainer.save_checkpoint(episode, is_best=is_best)\n",
        "                \n",
        "            if (episode + 1) % GLOBAL_CONFIG[\"DRIVE_SYNC_EVERY\"] == 0:\n",
        "                # Sync last & best\n",
        "                sync_checkpoint_to_drive(str(LOCAL_PHASE_DIR / \"last_checkpoint.pth\"), PHASE_CHECKPOINT_DIR, \"last_model.pth\")\n",
        "                if (LOCAL_PHASE_DIR / \"best_model.pth\").exists():\n",
        "                    sync_checkpoint_to_drive(str(LOCAL_PHASE_DIR / \"best_model.pth\"), PHASE_CHECKPOINT_DIR)\n",
        "                print(f\"\u2601\ufe0f Sync Drive Ep {episode}\")\n",
        "\n",
        "            # Visualizaci\u00f3n simple\n",
        "            if (episode + 1) % 10 == 0:\n",
        "                print(f\"Ep {episode+1}/{phase_cfg['TOTAL_EPISODES']} | Loss: {loss:.4f} | {monitor.get_status_str().splitlines()[0]}\")\n",
        "        \n",
        "        # Fin de Fase\n",
        "        print(f\"\\n\u2705 FASE {PHASE_NAME} COMPLETADA\")\n",
        "        \n",
        "        # Guardar marcador de completado\n",
        "        with open(final_marker, 'w') as f:\n",
        "            f.write(f\"Completed at {datetime.now()}\")\n",
        "            \n",
        "        # Sync final\n",
        "        sync_checkpoint_to_drive(str(LOCAL_PHASE_DIR / \"best_model.pth\"), PHASE_CHECKPOINT_DIR)\n",
        "        \n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\n\ud83d\uded1 Entrenamiento interrumpido.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"\\n\u274c Error en fase {PHASE_NAME}: {e}\")\n",
        "        raise e\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"\ud83c\udfc1 ENTRENAMIENTO FINALIZADO\")\n",
        "print(\"=\"*70)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.trainers.qc_trainer_v4 import QC_Trainer_v4\n",
        "from src.model_loader import instantiate_model, load_weights, load_checkpoint_data\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output, display\n",
        "import json\n",
        "\n",
        "# Inicializar monitor de recursos\n",
        "monitor = ResourceMonitor(max_training_hours=exp_cfg.MAX_TRAINING_HOURS)\n",
        "\n",
        "# Instanciar modelo\n",
        "model = instantiate_model(exp_cfg)\n",
        "\n",
        "# Cargar pesos si hay checkpoint\n",
        "if checkpoint_path:\n",
        "    ckpt_data = load_checkpoint_data(checkpoint_path)\n",
        "    if ckpt_data:\n",
        "        load_weights(model, ckpt_data)\n",
        "\n",
        "# Inicializar trainer\n",
        "trainer = QC_Trainer_v4(\n",
        "    experiment_name=exp_cfg.EXPERIMENT_NAME,\n",
        "    model=model,\n",
        "    device=device,\n",
        "    lr=exp_cfg.LR_RATE_M,\n",
        "    grid_size=exp_cfg.GRID_SIZE_TRAINING,\n",
        "    qca_steps=exp_cfg.QCA_STEPS_TRAINING,\n",
        "    gamma_decay=exp_cfg.GAMMA_DECAY,\n",
        "    max_checkpoints_to_keep=exp_cfg.MAX_CHECKPOINTS_TO_KEEP\n",
        ")\n",
        "\n",
        "# Tracking de m\u00e9tricas\n",
        "training_log = {\n",
        "    \"experiment_name\": exp_cfg.EXPERIMENT_NAME,\n",
        "    \"config\": EXPERIMENT_CONFIG,\n",
        "    \"episodes\": [],\n",
        "    \"losses\": [],\n",
        "    \"metrics\": [],\n",
        "    \"checkpoints_saved\": []\n",
        "}\n",
        "\n",
        "# Funci\u00f3n auxiliar para guardar en Drive\n",
        "def sync_checkpoint_to_drive(local_path, episode_num):\n",
        "    \"\"\"Copia checkpoint local a Drive\"\"\"\n",
        "    try:\n",
        "        import shutil\n",
        "        filename = Path(local_path).name\n",
        "        drive_path = EXPERIMENT_CHECKPOINT_DIR / filename\n",
        "        shutil.copy2(local_path, drive_path)\n",
        "        logger.info(f\"\ud83d\udcbe Checkpoint sincronizado a Drive: {filename}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"\u274c Error sincronizando a Drive: {e}\")\n",
        "        return False\n",
        "\n",
        "# ENTRENAMIENTO PROGRESIVO\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"\ud83c\udfaf INICIANDO ENTRENAMIENTO PROGRESIVO\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "try:\n",
        "    for episode in range(resume_from_episode, exp_cfg.TOTAL_EPISODES):\n",
        "        \n",
        "        # Verificar l\u00edmite de tiempo ANTES de empezar episodio\n",
        "        if monitor.should_stop():\n",
        "            print(\"\\n\u23f0 \u00a1L\u00cdMITE DE TIEMPO ALCANZADO!\")\n",
        "            print(\"\ud83d\udcbe Guardando checkpoint de emergencia...\")\n",
        "            \n",
        "            # Guardar checkpoint de emergencia\n",
        "            trainer.save_checkpoint(episode, is_best=False)\n",
        "            \n",
        "            # Sincronizar a Drive\n",
        "            last_checkpoint = LOCAL_CHECKPOINT_DIR / \"last_model.pth\"\n",
        "            if last_checkpoint.exists():\n",
        "                sync_checkpoint_to_drive(str(last_checkpoint), episode)\n",
        "            \n",
        "            print(f\"\u2705 Entrenamiento detenido en episodio {episode}\")\n",
        "            print(f\"\ud83d\udcca Progreso: {episode}/{exp_cfg.TOTAL_EPISODES} episodios ({episode/exp_cfg.TOTAL_EPISODES*100:.1f}%)\")\n",
        "            break\n",
        "        \n",
        "        # Entrenar episodio\n",
        "        epoch_result = trainer.train_episode(episode)\n",
        "        \n",
        "        # Registrar m\u00e9tricas\n",
        "        training_log[\"episodes\"].append(episode)\n",
        "        training_log[\"losses\"].append(epoch_result.get(\"loss_total\", 0))\n",
        "        training_log[\"metrics\"].append(epoch_result)\n",
        "        \n",
        "        # Guardar checkpoint peri\u00f3dicamente\n",
        "        if (episode + 1) % exp_cfg.SAVE_EVERY_EPISODES == 0:\n",
        "            is_best = (episode + 1) % (exp_cfg.SAVE_EVERY_EPISODES * 5) == 0  # Cada 50 eps = best candidate\n",
        "            trainer.save_checkpoint(episode, is_best=is_best)\n",
        "            training_log[\"checkpoints_saved\"].append(episode)\n",
        "        \n",
        "        # Sincronizar a Drive peri\u00f3dicamente\n",
        "        if (episode + 1) % exp_cfg.DRIVE_SYNC_EVERY == 0:\n",
        "            print(f\"\\n\ud83d\udcbe Sincronizando checkpoint a Drive (episodio {episode})...\")\n",
        "            last_checkpoint = LOCAL_CHECKPOINT_DIR / \"last_model.pth\"\n",
        "            if last_checkpoint.exists():\n",
        "                sync_checkpoint_to_drive(str(last_checkpoint), episode)\n",
        "            \n",
        "            # Tambi\u00e9n sincronizar mejores modelos\n",
        "            best_checkpoint = LOCAL_CHECKPOINT_DIR / \"best_model.pth\"\n",
        "            if best_checkpoint.exists():\n",
        "                sync_checkpoint_to_drive(str(best_checkpoint), episode)\n",
        "        \n",
        "        # Visualizaci\u00f3n cada 10 episodios\n",
        "        if (episode + 1) % 10 == 0:\n",
        "            clear_output(wait=True)\n",
        "            \n",
        "            print(\"=\" * 70)\n",
        "            print(f\"\ud83d\udcc8 PROGRESO: Episodio {episode + 1}/{exp_cfg.TOTAL_EPISODES} ({(episode+1)/exp_cfg.TOTAL_EPISODES*100:.1f}%)\")\n",
        "            print(\"=\" * 70)\n",
        "            \n",
        "            # Mostrar recursos\n",
        "            print(monitor.get_status_str())\n",
        "            \n",
        "            # Mostrar \u00faltimas m\u00e9tricas\n",
        "            if len(training_log[\"losses\"]) > 0:\n",
        "                recent_losses = training_log[\"losses\"][-10:]\n",
        "                avg_loss = sum(recent_losses) / len(recent_losses)\n",
        "                print(f\"\\n\ud83d\udcca M\u00c9TRICAS (\u00faltimos 10 episodios):\")\n",
        "                print(f\"  Loss promedio: {avg_loss:.4f}\")\n",
        "                print(f\"  Loss actual: {training_log['losses'][-1]:.4f}\")\n",
        "            \n",
        "            # Gr\u00e1fico de p\u00e9rdida\n",
        "            if len(training_log[\"episodes\"]) > 1:\n",
        "                plt.figure(figsize=(10, 4))\n",
        "                plt.plot(training_log[\"episodes\"], training_log[\"losses\"], 'b-', alpha=0.6)\n",
        "                plt.xlabel('Episodio')\n",
        "                plt.ylabel('Loss')\n",
        "                plt.title(f'{exp_cfg.EXPERIMENT_NAME} - Progreso de Entrenamiento')\n",
        "                plt.grid(True, alpha=0.3)\n",
        "                plt.tight_layout()\n",
        "                plt.show()\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"\u2705 ENTRENAMIENTO COMPLETADO EXITOSAMENTE\")\n",
        "    print(\"=\" * 70)\n",
        "    \n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n\u26a0\ufe0f Entrenamiento interrumpido por usuario\")\n",
        "    print(\"\ud83d\udcbe Guardando checkpoint...\")\n",
        "    trainer.save_checkpoint(episode, is_best=False)\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c ERROR DURANTE ENTRENAMIENTO: {e}\")\n",
        "    logger.error(f\"Error: {e}\", exc_info=True)\n",
        "    print(\"\ud83d\udcbe Guardando checkpoint de emergencia...\")\n",
        "    try:\n",
        "        trainer.save_checkpoint(episode, is_best=False)\n",
        "    except:\n",
        "        pass\n",
        "    raise\n",
        "\n",
        "finally:\n",
        "    # Guardar log de entrenamiento en Drive\n",
        "    log_file = EXPERIMENT_LOG_DIR / f\"training_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
        "    with open(log_file, 'w') as f:\n",
        "        json.dump(training_log, f, indent=2)\n",
        "    print(f\"\\n\ud83d\udcc4 Log guardado en: {log_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udcca Secci\u00f3n 8: Visualizaci\u00f3n de Resultados Finales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "if len(training_log[\"episodes\"]) > 0:\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    \n",
        "    # 1. P\u00e9rdida total\n",
        "    axes[0, 0].plot(training_log[\"episodes\"], training_log[\"losses\"], 'b-', alpha=0.6, label='Loss')\n",
        "    axes[0, 0].set_xlabel('Episodio')\n",
        "    axes[0, 0].set_ylabel('Loss Total')\n",
        "    axes[0, 0].set_title('Evoluci\u00f3n de la P\u00e9rdida')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    axes[0, 0].legend()\n",
        "    \n",
        "    # 2. M\u00e9tricas individuales (si est\u00e1n disponibles)\n",
        "    if len(training_log[\"metrics\"]) > 0 and \"survival_rate\" in training_log[\"metrics\"][0]:\n",
        "        survival_rates = [m.get(\"survival_rate\", 0) for m in training_log[\"metrics\"]]\n",
        "        axes[0, 1].plot(training_log[\"episodes\"], survival_rates, 'g-', alpha=0.6, label='Survival Rate')\n",
        "        axes[0, 1].set_xlabel('Episodio')\n",
        "        axes[0, 1].set_ylabel('Survival Rate')\n",
        "        axes[0, 1].set_title('Tasa de Supervivencia')\n",
        "        axes[0, 1].grid(True, alpha=0.3)\n",
        "        axes[0, 1].legend()\n",
        "    \n",
        "    # 3. Checkpoints guardados\n",
        "    axes[1, 0].scatter(training_log[\"checkpoints_saved\"], \n",
        "                      [1] * len(training_log[\"checkpoints_saved\"]), \n",
        "                      c='red', marker='|', s=100, label='Checkpoint')\n",
        "    axes[1, 0].set_xlabel('Episodio')\n",
        "    axes[1, 0].set_yticks([])\n",
        "    axes[1, 0].set_title('Checkpoints Guardados')\n",
        "    axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "    axes[1, 0].legend()\n",
        "    \n",
        "    # 4. Histograma de p\u00e9rdidas\n",
        "    axes[1, 1].hist(training_log[\"losses\"], bins=30, alpha=0.7, color='purple', edgecolor='black')\n",
        "    axes[1, 1].set_xlabel('Loss')\n",
        "    axes[1, 1].set_ylabel('Frecuencia')\n",
        "    axes[1, 1].set_title('Distribuci\u00f3n de P\u00e9rdidas')\n",
        "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig(EXPERIMENT_LOG_DIR / 'training_summary.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n\ud83d\udcca Gr\u00e1ficos guardados en: {EXPERIMENT_LOG_DIR / 'training_summary.png'}\")\n",
        "else:\n",
        "    print(\"\u26a0\ufe0f No hay datos de entrenamiento para visualizar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udce6 Secci\u00f3n 9: Exportaci\u00f3n y Finalizaci\u00f3n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.model_loader import load_model\n",
        "import shutil\n",
        "\n",
        "print(\"\ud83d\udce4 Exportando modelo final...\\n\")\n",
        "\n",
        "# Encontrar mejor checkpoint\n",
        "best_checkpoint = LOCAL_CHECKPOINT_DIR / \"best_model.pth\"\n",
        "if not best_checkpoint.exists():\n",
        "    best_checkpoint = find_latest_checkpoint(LOCAL_CHECKPOINT_DIR)\n",
        "\n",
        "if best_checkpoint:\n",
        "    print(f\"\ud83d\udce5 Cargando modelo desde: {Path(best_checkpoint).name}\")\n",
        "    \n",
        "    # Cargar modelo\n",
        "    model = load_model(exp_cfg, str(best_checkpoint))\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    # Exportar a TorchScript\n",
        "    example_input = torch.randn(1, 2 * exp_cfg.MODEL_PARAMS.d_state, \n",
        "                                exp_cfg.GRID_SIZE_TRAINING, \n",
        "                                exp_cfg.GRID_SIZE_TRAINING, device=device)\n",
        "    \n",
        "    torchscript_path = DRIVE_EXPORTS_DIR / f\"{exp_cfg.EXPERIMENT_NAME}_model.pt\"\n",
        "    \n",
        "    try:\n",
        "        traced_model = torch.jit.trace(model, example_input, strict=False)\n",
        "        traced_model.save(str(torchscript_path))\n",
        "        print(f\"\u2705 Modelo TorchScript exportado: {torchscript_path.name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\u26a0\ufe0f Error exportando TorchScript: {e}\")\n",
        "    \n",
        "    # Copiar mejor checkpoint a Drive\n",
        "    drive_best_checkpoint = EXPERIMENT_CHECKPOINT_DIR / \"best_model_FINAL.pth\"\n",
        "    shutil.copy2(best_checkpoint, drive_best_checkpoint)\n",
        "    print(f\"\u2705 Mejor checkpoint copiado: {drive_best_checkpoint.name}\")\n",
        "    \n",
        "    # Generar reporte de entrenamiento\n",
        "    report_path = EXPERIMENT_LOG_DIR / f\"{exp_cfg.EXPERIMENT_NAME}_REPORT.md\"\n",
        "    \n",
        "    with open(report_path, 'w') as f:\n",
        "        f.write(f\"# Reporte de Entrenamiento: {exp_cfg.EXPERIMENT_NAME}\\n\\n\")\n",
        "        f.write(f\"**Fecha:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "        f.write(f\"## Configuraci\u00f3n\\n\\n\")\n",
        "        f.write(f\"```json\\n{json.dumps(EXPERIMENT_CONFIG, indent=2)}\\n```\\n\\n\")\n",
        "        f.write(f\"## Resultados\\n\\n\")\n",
        "        f.write(f\"- **Total de episodios completados:** {len(training_log['episodes'])}\\n\")\n",
        "        f.write(f\"- **Checkpoints guardados:** {len(training_log['checkpoints_saved'])}\\n\")\n",
        "        \n",
        "        if len(training_log[\"losses\"]) > 0:\n",
        "            f.write(f\"- **Loss final:** {training_log['losses'][-1]:.4f}\\n\")\n",
        "            f.write(f\"- **Loss promedio:** {np.mean(training_log['losses']):.4f}\\n\")\n",
        "            f.write(f\"- **Loss m\u00ednimo:** {min(training_log['losses']):.4f}\\n\")\n",
        "        \n",
        "        f.write(f\"\\n## Archivos\\n\\n\")\n",
        "        f.write(f\"- Mejor modelo: `{drive_best_checkpoint.name}`\\n\")\n",
        "        f.write(f\"- TorchScript: `{torchscript_path.name}`\\n\")\n",
        "        f.write(f\"- Logs: `{EXPERIMENT_LOG_DIR.name}`\\n\")\n",
        "    \n",
        "    print(f\"\u2705 Reporte generado: {report_path.name}\")\n",
        "    \n",
        "else:\n",
        "    print(\"\u26a0\ufe0f No se encontr\u00f3 checkpoint para exportar\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"\ud83c\udf89 EXPORTACI\u00d3N COMPLETADA\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"\\n\ud83d\udcc1 Todos los archivos est\u00e1n en Drive:\")\n",
        "print(f\"  - Checkpoints: {EXPERIMENT_CHECKPOINT_DIR}\")\n",
        "print(f\"  - Logs: {EXPERIMENT_LOG_DIR}\")\n",
        "print(f\"  - Exports: {DRIVE_EXPORTS_DIR}\")\n",
        "print(\"\\n\u2705 Puedes cerrar el notebook. Todo est\u00e1 guardado en Drive.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## \ud83d\udcdd Notas Finales\n",
        "\n",
        "### \u2705 Checklist de Verificaci\u00f3n\n",
        "\n",
        "- \u2713 Checkpoints guardados en Drive cada 50 episodios\n",
        "- \u2713 Logs de entrenamiento persistidos\n",
        "- \u2713 Modelo final exportado a TorchScript\n",
        "- \u2713 Reporte de entrenamiento generado\n",
        "- \u2713 Auto-recuperaci\u00f3n configurada para pr\u00f3xima sesi\u00f3n\n",
        "\n",
        "### \ud83d\udd04 Para Continuar Entrenamiento\n",
        "\n",
        "1. Ejecutar este notebook de nuevo\n",
        "2. Mantener `AUTO_RESUME = True`\n",
        "3. Ajustar `TOTAL_EPISODES` si quieres m\u00e1s episodios\n",
        "4. El notebook detectar\u00e1 autom\u00e1ticamente el \u00faltimo checkpoint en Drive\n",
        "\n",
        "### \ud83d\udcca Mejores Pr\u00e1cticas\n",
        "\n",
        "- **Colab Free**: Entrenar en sesiones de 6-8 horas\n",
        "- **Colab Pro**: Sesiones de 12-20 horas\n",
        "- **Kaggle**: Aprovechar las 30 horas semanales\n",
        "- Siempre verificar que Drive est\u00e9 sincronizando correctamente\n",
        "- Revisar gr\u00e1ficos cada 50-100 episodios para detectar problemas\n",
        "\n",
        "---\n",
        "\n",
        "**\u00a1Feliz entrenamiento! \ud83d\ude80**"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
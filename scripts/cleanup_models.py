#!/usr/bin/env python3
"""
Script para limpiar modelos TorchScript defectuosos y checkpoints.
Verifica que los modelos .pt se puedan cargar correctamente.
"""
import os
import sys
import logging
import torch
import json
from pathlib import Path

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Agregar directorio ra√≠z al path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from src import config as global_cfg

def check_torchscript_model(model_path: Path) -> bool:
    """Verifica si un modelo TorchScript se puede cargar correctamente."""
    try:
        # Intentar cargar en CPU primero
        model = torch.jit.load(str(model_path), map_location='cpu')
        model.eval()
        
        # Intentar hacer un forward dummy para verificar que funciona
        with torch.no_grad():
            # Crear input dummy (necesitamos saber el tama√±o)
            # Por defecto, asumimos [1, 2*d_state, 64, 64]
            dummy_input = torch.randn(1, 16, 64, 64)  # 2*d_state con d_state=8
            
            try:
                output = model(dummy_input)
                # Si es tupla (ConvLSTM), verificar el primer elemento
                if isinstance(output, tuple):
                    if len(output) == 0:
                        logger.warning(f"  ‚ö†Ô∏è Modelo devuelve tupla vac√≠a: {model_path.name}")
                        return False
                    output = output[0]
                
                # Verificar que el output es un tensor v√°lido
                if not isinstance(output, torch.Tensor):
                    logger.warning(f"  ‚ö†Ô∏è Modelo no devuelve tensor: {model_path.name}")
                    return False
                    
                logger.info(f"  ‚úÖ Modelo v√°lido: {model_path.name}")
                return True
            except Exception as e:
                logger.warning(f"  ‚ö†Ô∏è Error en forward pass: {model_path.name} - {str(e)[:100]}")
                return False
    except Exception as e:
        logger.error(f"  ‚ùå Error cargando modelo: {model_path.name} - {str(e)[:100]}")
        return False

def check_checkpoint(checkpoint_path: Path) -> bool:
    """Verifica si un checkpoint se puede cargar correctamente."""
    try:
        checkpoint = torch.load(str(checkpoint_path), map_location='cpu')
        
        # Verificar que tiene estructura v√°lida
        if isinstance(checkpoint, dict):
            # Debe tener al menos 'model_state_dict' o 'state_dict'
            has_state = 'model_state_dict' in checkpoint or 'state_dict' in checkpoint
            if not has_state and len(checkpoint) == 0:
                logger.warning(f"  ‚ö†Ô∏è Checkpoint vac√≠o: {checkpoint_path.name}")
                return False
        elif not isinstance(checkpoint, dict):
            # Si no es dict, asumir que es directamente el state_dict
            if checkpoint is None:
                logger.warning(f"  ‚ö†Ô∏è Checkpoint None: {checkpoint_path.name}")
                return False
        
        logger.info(f"  ‚úÖ Checkpoint v√°lido: {checkpoint_path.name}")
        return True
    except Exception as e:
        logger.error(f"  ‚ùå Error cargando checkpoint: {checkpoint_path.name} - {str(e)[:100]}")
        return False

def cleanup_models():
    """Limpia modelos TorchScript defectuosos."""
    torchscript_dir = PROJECT_ROOT / "output" / "torchscript_models"
    
    if not torchscript_dir.exists():
        logger.info(f"üìÅ Directorio de modelos TorchScript no existe: {torchscript_dir}")
        return
    
    logger.info(f"\nüîç Verificando modelos TorchScript en: {torchscript_dir}")
    
    model_files = list(torchscript_dir.glob("*.pt"))
    if not model_files:
        logger.info("  No se encontraron modelos .pt")
        return
    
    logger.info(f"  Encontrados {len(model_files)} modelos")
    
    valid_models = []
    invalid_models = []
    
    for model_path in model_files:
        if check_torchscript_model(model_path):
            valid_models.append(model_path)
        else:
            invalid_models.append(model_path)
    
    logger.info(f"\nüìä Resumen:")
    logger.info(f"  ‚úÖ Modelos v√°lidos: {len(valid_models)}")
    logger.info(f"  ‚ùå Modelos defectuosos: {len(invalid_models)}")
    
    if invalid_models:
        logger.info(f"\nüóëÔ∏è  Eliminando {len(invalid_models)} modelos defectuosos...")
        for model_path in invalid_models:
            try:
                model_path.unlink()
                logger.info(f"  ‚úÖ Eliminado: {model_path.name}")
            except Exception as e:
                logger.error(f"  ‚ùå Error eliminando {model_path.name}: {e}")

def cleanup_checkpoints():
    """Limpia checkpoints defectuosos."""
    checkpoint_dir = PROJECT_ROOT / "output" / "training_checkpoints"
    
    if not checkpoint_dir.exists():
        logger.info(f"üìÅ Directorio de checkpoints no existe: {checkpoint_dir}")
        return
    
    logger.info(f"\nüîç Verificando checkpoints en: {checkpoint_dir}")
    
    checkpoint_files = list(checkpoint_dir.glob("**/*.pth"))
    if not checkpoint_files:
        logger.info("  No se encontraron checkpoints .pth")
        return
    
    logger.info(f"  Encontrados {len(checkpoint_files)} checkpoints")
    
    valid_checkpoints = []
    invalid_checkpoints = []
    
    for checkpoint_path in checkpoint_files:
        if check_checkpoint(checkpoint_path):
            valid_checkpoints.append(checkpoint_path)
        else:
            invalid_checkpoints.append(checkpoint_path)
    
    logger.info(f"\nüìä Resumen:")
    logger.info(f"  ‚úÖ Checkpoints v√°lidos: {len(valid_checkpoints)}")
    logger.info(f"  ‚ùå Checkpoints defectuosos: {len(invalid_checkpoints)}")
    
    if invalid_checkpoints:
        logger.info(f"\nüóëÔ∏è  Eliminando {len(invalid_checkpoints)} checkpoints defectuosos...")
        for checkpoint_path in invalid_checkpoints:
            try:
                checkpoint_path.unlink()
                logger.info(f"  ‚úÖ Eliminado: {checkpoint_path.name}")
            except Exception as e:
                logger.error(f"  ‚ùå Error eliminando {checkpoint_path.name}: {e}")

def delete_all_checkpoints():
    """Elimina todos los checkpoints (solicitado por el usuario)."""
    checkpoint_dir = PROJECT_ROOT / "output" / "training_checkpoints"
    
    if not checkpoint_dir.exists():
        logger.info(f"üìÅ Directorio de checkpoints no existe: {checkpoint_dir}")
        return
    
    logger.info(f"\nüóëÔ∏è  Eliminando TODOS los checkpoints en: {checkpoint_dir}")
    
    checkpoint_files = list(checkpoint_dir.glob("**/*.pth"))
    
    if not checkpoint_files:
        logger.info("  No se encontraron checkpoints para eliminar")
        return
    
    logger.info(f"  Encontrados {len(checkpoint_files)} checkpoints")
    
    for checkpoint_path in checkpoint_files:
        try:
            checkpoint_path.unlink()
            logger.info(f"  ‚úÖ Eliminado: {checkpoint_path.name}")
        except Exception as e:
            logger.error(f"  ‚ùå Error eliminando {checkpoint_path.name}: {e}")
    
    logger.info(f"\n‚úÖ Eliminados {len(checkpoint_files)} checkpoints")

def main():
    """Funci√≥n principal."""
    import argparse
    
    parser = argparse.ArgumentParser(description="Limpiar modelos TorchScript y checkpoints defectuosos")
    parser.add_argument("--delete-all-checkpoints", action="store_true",
                       help="Eliminar TODOS los checkpoints (no solo los defectuosos)")
    parser.add_argument("--skip-models", action="store_true",
                       help="Saltar verificaci√≥n de modelos")
    parser.add_argument("--skip-checkpoints", action="store_true",
                       help="Saltar verificaci√≥n de checkpoints")
    
    args = parser.parse_args()
    
    logger.info("üßπ Iniciando limpieza de modelos y checkpoints...")
    
    # Verificar y limpiar modelos TorchScript
    if not args.skip_models:
        cleanup_models()
    
    # Eliminar todos los checkpoints si se solicita
    if args.delete_all_checkpoints:
        delete_all_checkpoints()
    elif not args.skip_checkpoints:
        # Solo verificar y eliminar checkpoints defectuosos
        cleanup_checkpoints()
    
    logger.info("\n‚úÖ Limpieza completada")

if __name__ == "__main__":
    main()


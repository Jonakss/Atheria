# ComunicaciÃ³n Motor Nativo (C++) â†” Python â†” Frontend

## Arquitectura de ComunicaciÃ³n

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  C++ (LibTorch) â”‚
â”‚ atheria_core    â”‚
â”‚  .Engine        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ PyBind11
         â”‚ (tensors directos)
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Python        â”‚
â”‚NativeEngineWrap â”‚
â”‚     per         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ g_state['motor']
         â”‚ motor.evolve_internal_state()
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python Backend  â”‚
â”‚pipeline_server  â”‚
â”‚simulation_loop  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ WebSocket (JSON)
         â”‚ inference_status_update
         â”‚ compile_status.is_native
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚
â”‚ React/TypeScriptâ”‚
â”‚  WebSocketCtx   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Flujo de Datos

### 1. Carga del Motor (`handle_load_experiment`)

**UbicaciÃ³n**: `src/pipeline_server.py` (lÃ­nea ~720)

**Proceso**:

1. **VerificaciÃ³n de ConfiguraciÃ³n**:
   ```python
   use_native_engine = getattr(global_cfg, 'USE_NATIVE_ENGINE', True)
   ```

2. **Intento de Carga Nativo**:
   - Verifica si `atheria_core` estÃ¡ disponible
   - Busca modelo JIT (`.pt`) o exporta automÃ¡ticamente desde checkpoint
   - Instancia `NativeEngineWrapper` si el modelo JIT existe
   - Carga el modelo en el motor C++: `motor.native_engine.load_model(jit_path)`

3. **Fallback a Python**:
   - Si falla la carga nativa, usa `Aetheria_Motor` (Python)
   - Compila con `torch.compile()` si estÃ¡ disponible

4. **NotificaciÃ³n al Frontend**:
   ```python
   compile_status = {
       "is_compiled": True,  # Nativo siempre "compilado"
       "is_native": True,    # â† INDICADOR PRINCIPAL
       "model_name": "Native Engine (C++)",
       "compiles_enabled": True
   }
   
   await broadcast({
       "type": "inference_status_update",
       "payload": {
           "status": "paused",
           "model_loaded": True,
           "experiment_name": exp_name,
           "compile_status": compile_status  # â† InformaciÃ³n del motor
       }
   })
   ```

### 2. EvoluciÃ³n del Estado (`simulation_loop`)

**UbicaciÃ³n**: `src/pipeline_server.py` (lÃ­nea ~127)

**Proceso para Motor Nativo**:

```python
# LÃ­nea ~213
g_state['motor'].evolve_internal_state()
```

**Motor Nativo** (`NativeEngineWrapper.evolve_internal_state`):
```python
# src/engines/native_engine_wrapper.py (lÃ­nea ~97)
def evolve_internal_state(self):
    # 1. Ejecutar paso nativo en C++ (TODO en C++)
    particle_count = self.native_engine.step_native()
    
    # 2. Convertir estado disperso â†’ denso para visualizaciÃ³n
    self._update_dense_state_from_sparse()
```

**Flujo Detallado del Motor Nativo**:

1. **C++ (`step_native`)**:
   - Itera sobre `SparseMap` de partÃ­culas
   - Genera vacÃ­o cuÃ¡ntico con `HarmonicVacuum` para vecinos
   - Batchea inputs para el modelo
   - Ejecuta inferencia: `model.forward({input})`
   - Actualiza partÃ­culas en el mapa disperso
   - **TODO ocurre en C++/GPU sin pasar por Python**

2. **Python (ConversiÃ³n para VisualizaciÃ³n)**:
   ```python
   # _update_dense_state_from_sparse (lÃ­nea ~111)
   # Itera sobre todo el grid (256x256 por defecto)
   for y in range(grid_size):
       for x in range(grid_size):
           coord = atheria_core.Coord3D(x, y, 0)
           state_tensor = self.native_engine.get_state_at(coord)
           # Copiar a grid denso para frontend
           self.state.psi[0, y, x] = state_tensor
   ```
   
   **NOTA**: Esta conversiÃ³n es el Ãºnico cuello de botella. El motor nativo ejecuta la fÃ­sica 250-400x mÃ¡s rÃ¡pido, pero la conversiÃ³n dispersoâ†’denso toma tiempo.

3. **VisualizaciÃ³n**:
   - `simulation_loop` obtiene `motor.state.psi` (denso)
   - Calcula visualizaciones con `get_visualization_data()`
   - EnvÃ­a frame JSON al frontend via WebSocket

**Motor Python** (`Aetheria_Motor.evolve_internal_state`):
```python
# Todo en Python/PyTorch
# MÃ¡s lento pero sin conversiÃ³n de formato
```

### 3. RecepciÃ³n en Frontend

**UbicaciÃ³n**: `frontend/src/context/WebSocketContext.tsx`

**Procesamiento**:

```typescript
// Manejo de mensaje inference_status_update
case 'inference_status_update':
    const payload = message.payload;
    setInferenceStatus(payload.status);
    
    // compile_status contiene informaciÃ³n del motor
    if (payload.compile_status) {
        const { is_native, model_name, is_compiled } = payload.compile_status;
        // âš ï¸ ACTUALMENTE NO SE MUESTRA EN LA UI
        // Pero estÃ¡ disponible en el contexto
    }
    break;
```

## CÃ³mo Verificar quÃ© Motor EstÃ¡ en Uso

### 1. **Desde el Backend (Logs)**:

Cuando cargas un experimento, busca en los logs:

```
âœ… Motor nativo (C++) cargado exitosamente con modelo JIT
âš¡ Motor nativo cargado (250-400x mÃ¡s rÃ¡pido)
```

O:

```
Usando motor Python tradicional (Aetheria_Motor)
âœ… Modelo compilado con torch.compile()
```

### 2. **Desde la ConfiguraciÃ³n**:

```python
# src/config.py (lÃ­nea ~74)
USE_NATIVE_ENGINE = True  # Por defecto True
```

Si es `True` y hay modelo JIT disponible, se usarÃ¡ el motor nativo.

### 3. **Desde el Frontend (Actualmente No Visible)**:

El frontend recibe `compile_status.is_native` pero **NO se muestra en la UI actualmente**.

## Problemas Identificados

### 1. **Falta Indicador Visual en Frontend**

El frontend recibe `compile_status.is_native` pero no lo muestra. DeberÃ­amos:

- Agregar un badge en `MainHeader` o `ExperimentInfo` que muestre "âš¡ Nativo" o "ğŸ Python"
- Mostrar esto en `ExperimentInfo.tsx` junto con otros detalles del modelo

### 2. **ConversiÃ³n Disperso â†’ Denso Costosa**

El `_update_dense_state_from_sparse()` itera sobre **todo el grid** (256x256 = 65,536 coordenadas) en cada paso. Esto puede ser mÃ¡s lento que la simulaciÃ³n misma.

**OptimizaciÃ³n Futura**:
- Solo convertir coordenadas activas
- Usar batching mÃ¡s agresivo
- Paralelizar la conversiÃ³n con multiprocessing

### 3. **Falta InformaciÃ³n de Rendimiento**

No hay forma de ver en tiempo real:
- CuÃ¡nto tiempo toma `step_native()` en C++
- CuÃ¡nto tiempo toma la conversiÃ³n dispersoâ†’denso
- FPS real de la simulaciÃ³n

## Mejoras Propuestas

1. **Agregar Indicador Visual**:
   ```typescript
   // ExperimentInfo.tsx
   {compileStatus?.is_native && (
       <Badge color="blue" variant="light">
           âš¡ Motor Nativo (C++)
       </Badge>
   )}
   ```

2. **Agregar MÃ©tricas de Rendimiento**:
   - Enviar timestamps de inicio/fin de cada paso
   - Calcular FPS en frontend
   - Mostrar en `MainHeader`

3. **Optimizar ConversiÃ³n**:
   - Solo convertir regiÃ³n visible (ROI)
   - Lazy loading de coordenadas fuera de pantalla
   - Cachear conversiones para coordenadas sin cambios

## Comandos Ãštiles

### Verificar si atheria_core estÃ¡ disponible:
```python
python3 -c "import atheria_core; print('âœ… Nativo disponible')"
```

### Exportar modelo manualmente a JIT:
```python
python3 scripts/export_model_to_jit.py
```

### Forzar uso de motor Python:
```python
# En src/config.py
USE_NATIVE_ENGINE = False
```


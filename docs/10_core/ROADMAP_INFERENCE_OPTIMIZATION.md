# üöÄ Roadmap: Inference Optimization & Serving

> **Objetivo:** Transformar Atheria de un prototipo de investigaci√≥n a una soluci√≥n de producci√≥n escalable mediante optimizaci√≥n de inferencia, servicio as√≠ncrono y cuantizaci√≥n.

**Estado:** üìÖ Planificaci√≥n  
**Documento Base:** [[INFERENCE_OPTIMIZATION_STRATEGIES]]

---

## üìã Resumen de Fases

| Fase | Nombre | Objetivo | Estado |
|------|--------|----------|--------|
| **Fase 1** | **Infraestructura As√≠ncrona** | Desacoplar servicio de inferencia | üî¥ Pendiente |
| **Fase 2** | **Compresi√≥n de Modelo** | Reducir VRAM y coste | üî¥ Pendiente |
| **Fase 3** | **Aceleraci√≥n de Grafo** | Maximizar throughput puro | üî¥ Pendiente |
| **Fase 4** | **Despliegue Productivo** | Escalar y monitorizar | üî¥ Pendiente |

---

## üõ†Ô∏è Fase 1: Infraestructura As√≠ncrona (LitServe)

**Meta:** Eliminar bloqueos en el bucle de simulaci√≥n y permitir concurrencia.

- [ ] **Migraci√≥n a LitServe**
  - [ ] Crear clase `AtheriaInferenceAPI` heredando de `LitAPI`
  - [ ] Implementar m√©todo `setup()` para carga de modelo Ley M
  - [ ] Implementar m√©todo `predict()` para inferencia de un paso
- [ ] **Gesti√≥n de Concurrencia**
  - [ ] Configurar `max_batch_size` y `batch_timeout` (e.g., 4 requests / 50ms)
  - [ ] Implementar manejo de colas para m√∫ltiples clientes WebSocket
- [ ] **Integraci√≥n con WebSocket Existente**
  - [ ] Adaptar `SimulationService` para usar el endpoint de LitServe (o integrarlo in-process)
  - [ ] Asegurar que el streaming de frames no se bloquee por la inferencia

## üìâ Fase 2: Compresi√≥n de Modelo (Quantization)

**Meta:** Reducir requisitos de hardware (A100 -> L4) sin perder emergencia.

- [ ] **Implementaci√≥n de NF4 (4-bit)**
  - [ ] Integrar `bitsandbytes` en el pipeline de carga de Ley M
  - [ ] Configurar `QuantizationConfig` para backbone del modelo
- [ ] **Validaci√≥n de Calidad**
  - [ ] Crear script de comparaci√≥n de fidelidad (FP16 vs NF4)
  - [ ] Verificar m√©tricas de emergencia (entrop√≠a, complejidad) en simulaci√≥n larga
- [ ] **Optimizaci√≥n de Memoria**
  - [ ] Medir reducci√≥n de VRAM
  - [ ] Ajustar batch size m√°ximo permitido con la nueva memoria disponible

## ‚ö° Fase 3: Aceleraci√≥n de Grafo (torch.compile)

**Meta:** Reducir overhead de Python y optimizar kernels de GPU.

- [ ] **Compilaci√≥n JIT**
  - [ ] Envolver modelo Ley M con `torch.compile(mode="reduce-overhead")`
  - [ ] Identificar y eliminar "graph breaks" en el c√≥digo del modelo
- [ ] **Optimizaci√≥n de Tensores**
  - [ ] Asegurar formas est√°ticas (static shapes) en los tensores de entrada
  - [ ] Pre-asignar memoria para buffers recurrentes
- [ ] **Benchmarking**
  - [ ] Medir latencia de inferencia (ms/step) antes y despu√©s
  - [ ] Comparar throughput (steps/sec)

## üöÄ Fase 4: Despliegue y Escalado

**Meta:** Infraestructura robusta para m√∫ltiples usuarios o simulaciones masivas.

- [ ] **Contenerizaci√≥n**
  - [ ] Actualizar Dockerfile para incluir dependencias de optimizaci√≥n (LitServe, bitsandbytes)
  - [ ] Crear imagen optimizada para inferencia (separada de entrenamiento)
- [ ] **Orquestaci√≥n**
  - [ ] Configurar despliegue en Lightning AI o Kubernetes
  - [ ] Implementar auto-escalado basado en profundidad de cola
- [ ] **Monitorizaci√≥n**
  - [ ] Exponer m√©tricas de inferencia (latencia, throughput, VRAM)
  - [ ] Integrar alertas de degradaci√≥n de rendimiento

---

## üîó Referencias

- [[INFERENCE_OPTIMIZATION_STRATEGIES]] - Estrategias detalladas
- [[EXTERNAL_RESEARCH_GEMINI_OPTIMIZATION_ANALYSIS]] - Investigaci√≥n original
- [[ROADMAP_PHASE_2]] - Relacionado: Motor Nativo C++ (otra v√≠a de optimizaci√≥n)

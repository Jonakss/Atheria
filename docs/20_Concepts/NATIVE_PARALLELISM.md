# ‚ö° Paralelismo en Motor Nativo (OpenMP)

## üìñ Concepto

El **Paralelismo en el Motor Nativo** se refiere a la capacidad del motor C++ de Atheria para procesar m√∫ltiples part√≠culas o regiones del espacio simult√°neamente utilizando m√∫ltiples hilos de CPU. Esto es crucial para escalar la simulaci√≥n a millones de part√≠culas activas.

## üõ†Ô∏è Implementaci√≥n T√©cnica

Utilizamos **OpenMP** (Open Multi-Processing), una API est√°ndar para programaci√≥n de memoria compartida en C++.

### Estrategia de Paralelizaci√≥n

La funci√≥n principal `step_native()` itera sobre todas las coordenadas activas para calcular su evoluci√≥n. Esta iteraci√≥n es "embarrassingly parallel" (vergonzosamente paralela), ya que el estado siguiente de una celda depende solo del estado actual de sus vecinos (que es inmutable durante el paso).

```cpp
#pragma omp parallel
{
    // Almacenamiento local por hilo (Thread-Local Storage)
    std::vector<Coord3D> local_batch_coords;
    // ...
    
    #pragma omp for schedule(dynamic)
    for (size_t i = 0; i < processed_coords.size(); i++) {
        // Procesamiento independiente de cada coordenada
        // ...
    }
    
    // Fusi√≥n segura (Critical Section)
    #pragma omp critical
    {
        // Unir resultados locales al mapa global del siguiente paso
    }
}
```

### Componentes Clave

1.  **Thread-Local Storage:** Cada hilo mantiene sus propios buffers (`local_batch_coords`, `local_next_matter_map`) para evitar condiciones de carrera y contenci√≥n de bloqueo.
2.  **Batch Processing:** Las part√≠culas se agrupan en lotes (batches) para aprovechar la eficiencia de inferencia de PyTorch/LibTorch.
3.  **Dynamic Scheduling:** Usamos `schedule(dynamic)` porque la carga de trabajo por part√≠cula puede variar (algunas pueden estar vac√≠as o requerir menos c√≥mputo).

## ‚ö†Ô∏è Consideraciones de Seguridad (Deadlocks)

Existe un riesgo conocido de **deadlocks** al combinar OpenMP con LibTorch (PyTorch C++ API), ya que ambos intentan gestionar el pool de hilos.

- **S√≠ntoma:** La simulaci√≥n se congela completamente.
- **Soluci√≥n:** Ajustar `OMP_NUM_THREADS` y `torch::set_num_threads` para evitar conflictos. Generalmente, queremos que OpenMP maneje el paralelismo de alto nivel (part√≠culas) y PyTorch use un solo hilo por operaci√≥n de inferencia (ya que estamos paralelizando *fuera* de PyTorch).

## üìä Impacto en Rendimiento

- **Esperado:** Mejora lineal con el n√∫mero de n√∫cleos f√≠sicos (hasta cierto punto de saturaci√≥n de memoria).
- **Cuello de Botella:** La fusi√≥n final de resultados (`#pragma omp critical`) y la transferencia de memoria.

## üîó Referencias

- [[SPARSE_ENGINE_ACTIVE_NEIGHBORS]] - C√≥mo funcionan los vecinos activos
- [[SPARSE_ARCHITECTURE_V4]] - Arquitectura sparse general
- [[NATIVE_ENGINE_DEVICE_CONFIG]] - Configuraci√≥n de device
- [[CUDA_CONFIGURATION]] - Configuraci√≥n de CUDA

## Tags

#native-engine #parallelism #openmp #performance #cpp

# Migraci√≥n de Experimentos: Python CUDA ‚Üí Motor Nativo C++

## Resumen

S√≠, puedes usar un experimento entrenado en **Python CUDA** con el **motor nativo C++** sin problemas. El sistema realiza la conversi√≥n autom√°ticamente cuando cargas el experimento.

## Proceso Autom√°tico

### Cuando cargas un experimento:

1. **B√∫squeda de modelo JIT**: El sistema busca un modelo TorchScript (`.pt`) ya exportado.
2. **Exportaci√≥n autom√°tica**: Si no existe, exporta autom√°ticamente el checkpoint de PyTorch (`.pth`) a TorchScript.
3. **Uso del motor nativo**: Una vez exportado, el motor nativo C++ puede usar el modelo.

### Ubicaci√≥n de archivos:

- **Checkpoints Python**: `output/training_checkpoints/<experiment_name>/checkpoint_*.pth`
- **Modelos JIT (TorchScript)**: `output/training_checkpoints/<experiment_name>/model_*.pt`

## Conversi√≥n Manual (Opcional)

Si quieres exportar manualmente un modelo a TorchScript:

```bash
python scripts/test_native_engine.py --experiment NOMBRE_EXPERIMENTO
```

Este script:
- Carga el checkpoint m√°s reciente del experimento
- Exporta el modelo a TorchScript usando `torch.jit.trace()` o `torch.jit.script()`
- Guarda el `.pt` en el directorio de checkpoints

## Compatibilidad

### ‚úÖ Compatible:

- **Arquitecturas soportadas**: Todas las arquitecturas de modelos est√°n soportadas (UNet, UNetUnitary, ConvLSTM, etc.)
- **Dispositivo**: Los modelos entrenados en CUDA se pueden usar en el motor nativo tanto en CPU como CUDA.
- **Pesos**: Los pesos del checkpoint se preservan completamente durante la exportaci√≥n.

### ‚ö†Ô∏è Limitaciones:

- **Estado del modelo**: El estado interno del modelo (si tiene memoria como ConvLSTM) se resetea al exportar.
- **Tama√±o de grid**: El modelo se exporta con el tama√±o de grid de inferencia (normalmente 256x256).

## Detalles T√©cnicos

### Proceso de Exportaci√≥n:

1. **Carga del checkpoint**: Se carga el modelo PyTorch desde `.pth` con los pesos entrenados.
2. **Modo evaluaci√≥n**: El modelo se pone en modo `eval()`.
3. **Ejemplo de entrada**: Se crea un tensor de ejemplo con el tama√±o de grid de inferencia.
4. **TorchScript export**: Se usa `torch.jit.trace()` (o `torch.jit.script()` como fallback).
5. **Guardado**: El modelo TorchScript se guarda como `.pt`.

### Verificaci√≥n:

El sistema verifica que:
- El modelo TorchScript se puede cargar correctamente.
- El forward pass funciona con el ejemplo de entrada.
- El modelo es compatible con el motor nativo C++.

## Uso en el Frontend

Cuando cargas un experimento desde el frontend:

1. Si existe un modelo JIT, se usa directamente con el motor nativo.
2. Si no existe, ver√°s la notificaci√≥n: "üì¶ Exportando modelo a TorchScript..."
3. Una vez exportado, el motor nativo se inicializa autom√°ticamente.

## Ventajas del Motor Nativo

- **Rendimiento**: 250-400x m√°s r√°pido que el motor Python.
- **Memoria**: Usa arquitectura dispersa (sparse) m√°s eficiente.
- **Escalabilidad**: Mejor manejo de grids grandes (256x256 o m√°s).

## Referencias

- `scripts/test_native_engine.py`: Funci√≥n `export_model_to_torchscript()`
- `src/pipelines/pipeline_server.py`: Funci√≥n `handle_load_experiment()` (l√≠nea ~1015)
- `src/engines/native_engine_wrapper.py`: Wrapper del motor nativo


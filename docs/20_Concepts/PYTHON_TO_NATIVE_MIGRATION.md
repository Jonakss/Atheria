# Migraci√≥n de Experimentos: Python CUDA ‚Üí Motor Nativo C++

## Resumen

S√≠, puedes usar un experimento entrenado en **Python CUDA** con el **motor nativo C++** sin problemas. El sistema realiza la conversi√≥n autom√°ticamente cuando cargas el experimento.

## Proceso Autom√°tico

### Cuando cargas un experimento:

1. **B√∫squeda de modelo JIT**: El sistema busca un modelo TorchScript (`.pt`) ya exportado.
2. **Exportaci√≥n autom√°tica**: Si no existe, exporta autom√°ticamente el checkpoint de PyTorch (`.pth`) a TorchScript.
3. **Uso del motor nativo**: Una vez exportado, el motor nativo C++ puede usar el modelo.

### Ubicaci√≥n de archivos:

- **Checkpoints Python**: `output/training_checkpoints/<experiment_name>/checkpoint_*.pth`
- **Modelos JIT (TorchScript)**: `output/training_checkpoints/<experiment_name>/model_*.pt`

## Conversi√≥n Manual (Opcional)

Si quieres exportar manualmente un modelo a TorchScript:

```bash
python scripts/test_native_engine.py --experiment NOMBRE_EXPERIMENTO
```

Este script:
- Carga el checkpoint m√°s reciente del experimento
- Exporta el modelo a TorchScript usando `torch.jit.trace()` o `torch.jit.script()`
- Guarda el `.pt` en el directorio de checkpoints

## Compatibilidad

### ‚úÖ Compatible:

- **Arquitecturas soportadas**: Todas las arquitecturas de modelos est√°n soportadas (UNet, UNetUnitary, ConvLSTM, etc.)
- **Dispositivo**: Los modelos entrenados en CUDA se pueden usar en el motor nativo tanto en CPU como CUDA.
- **Pesos**: Los pesos del checkpoint se preservan completamente durante la exportaci√≥n.

### ‚ö†Ô∏è Limitaciones:

- **Estado del modelo**: El estado interno del modelo (si tiene memoria como ConvLSTM) se resetea al exportar.
- **Tama√±o de grid**: El modelo se exporta con el tama√±o de grid de inferencia (normalmente 256x256).

## Detalles T√©cnicos

### Proceso de Exportaci√≥n:

1. **Carga del checkpoint**: Se carga el modelo PyTorch desde `.pth` con los pesos entrenados.
2. **Modo evaluaci√≥n**: El modelo se pone en modo `eval()`.
3. **Ejemplo de entrada**: Se crea un tensor de ejemplo con el tama√±o de grid de inferencia.
4. **TorchScript export**: Se usa `torch.jit.trace()` (o `torch.jit.script()` como fallback).
5. **Guardado**: El modelo TorchScript se guarda como `.pt`.

### Verificaci√≥n:

El sistema verifica que:
- El modelo TorchScript se puede cargar correctamente.
- El forward pass funciona con el ejemplo de entrada.
- El modelo es compatible con el motor nativo C++.

## Uso en el Frontend

Cuando cargas un experimento desde el frontend:

1. Si existe un modelo JIT, se usa directamente con el motor nativo.
2. Si no existe, ver√°s la notificaci√≥n: "üì¶ Exportando modelo a TorchScript..."
3. Una vez exportado, el motor nativo se inicializa autom√°ticamente.

## Ventajas del Motor Nativo

- **Rendimiento**: 250-400x m√°s r√°pido que el motor Python.
- **Memoria**: Usa arquitectura dispersa (sparse) m√°s eficiente.
- **Escalabilidad**: Mejor manejo de grids grandes (256x256 o m√°s).

## Referencias

- `scripts/test_native_engine.py`: Funci√≥n `export_model_to_torchscript()`
- `src/pipelines/pipeline_server.py`: Funci√≥n `handle_load_experiment()` (l√≠nea ~1015)
- `src/engines/native_engine_wrapper.py`: Wrapper del motor nativo

## Troubleshooting

### Problemas Comunes

#### 1. Bloqueo al cargar experimento (Timeout)
**S√≠ntoma**: La carga del experimento se queda colgada en "Cargando modelo TorchScript..." o falla con timeout.
**Causa**: El modelo JIT puede tardar en cargar, o hay un conflicto con CUDA.
**Soluci√≥n**:
- El sistema ahora intenta cargar primero en CPU y luego mover a CUDA.
- Si persiste, intenta reiniciar el servidor (`ath dev`).

#### 2. Part√≠culas "Fantasma" (Estado Vac√≠o)
**S√≠ntoma**: El experimento carga, pero al dar Play no se ve nada (pantalla negra/gris) y los logs dicen "0 part√≠culas activas".
**Causa**: Posible desincronizaci√≥n entre el `matter_map` de C++ y el wrapper de Python.
**Soluci√≥n**:
- Ejecuta el script de diagn√≥stico: `python test_native_state_recovery.py`
- Si el script falla, hay un problema en el motor nativo. Reportar como bug.
- Workaround: Usar el motor Python temporalmente (configurar `FORCE_NATIVE_ENGINE = False` en `config.py`).

#### 3. Error "Version Mismatch"
**S√≠ntoma**: Error al cargar `model_jit.pt` diciendo que fue guardado con una versi√≥n diferente de PyTorch.
**Soluci√≥n**:
- Borrar el archivo `.pt` antiguo: `rm output/training_checkpoints/<exp>/model_jit.pt`
- Recargar el experimento para forzar una re-exportaci√≥n con la versi√≥n actual.

## 2024-11-21 - Manejo Robusto de CUDA Out of Memory

### Contexto
Durante el entrenamiento de modelos grandes (especialmente UNetConvLSTM), se report√≥ un error de `torch.cuda.OutOfMemoryError` que deten√≠a completamente el entrenamiento, perdiendo todo el progreso. El error ocurr√≠a t√≠picamente despu√©s de varios episodios cuando la memoria CUDA se fragmentaba o acumulaba.

### Problema Resuelto

#### Antes
- No hab√≠a manejo de errores para OutOfMemoryError
- El entrenamiento se deten√≠a abruptamente sin guardar progreso
- No hab√≠a limpieza peri√≥dica de memoria CUDA
- La memoria se acumulaba durante episodios largos

#### Despu√©s
- ‚úÖ Manejo robusto de OutOfMemoryError con reintento autom√°tico
- ‚úÖ Limpieza peri√≥dica de cach√© CUDA durante entrenamiento
- ‚úÖ Guardado autom√°tico de checkpoint si error persistente
- ‚úÖ Recuperaci√≥n autom√°tica despu√©s de limpiar memoria

### Implementaci√≥n

#### 1. Manejo en `train_episode()` (QC_Trainer_v4)

**Archivo:** `src/trainers/qc_trainer_v4.py`

**Funci√≥n:** `train_episode()`

**Cambios:**
- Envuelve `loss.backward()` y `optimizer.step()` en try-except para capturar OutOfMemoryError
- Si ocurre error, limpia cach√© CUDA y reintenta una vez
- Limpieza peri√≥dica de cach√© CUDA cada 10 episodios (despu√©s de calcular p√©rdida)

**C√≥digo:**
```python
try:
    loss.backward()
    torch.nn.utils.clip_grad_norm_(self.motor.operator.parameters(), 1.0)
    self.optimizer.step()
except torch.cuda.OutOfMemoryError as e:
    # Limpiar cach√© y reintentar una vez
    logging.warning(f"‚ö†Ô∏è CUDA Out of Memory durante entrenamiento episodio {episode_num}. Limpiando cach√©...")
    torch.cuda.empty_cache()
    gc.collect()
    try:
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.motor.operator.parameters(), 1.0)
        self.optimizer.step()
        logging.info("‚úÖ Recuperado despu√©s de limpiar cach√© CUDA")
    except torch.cuda.OutOfMemoryError:
        logging.error(f"‚ùå CUDA Out of Memory persistente en episodio {episode_num}. Deteniendo entrenamiento.")
        raise

# Limpiar cach√© CUDA peri√≥dicamente (cada 10 episodios)
if episode_num % 10 == 0 and torch.cuda.is_available():
    torch.cuda.empty_cache()
```

#### 2. Manejo en Loop Principal de Entrenamiento

**Archivo:** `src/pipelines/pipeline_train.py`

**Funci√≥n:** `_run_v4_training_loop()`

**Cambios:**
- Captura OutOfMemoryError en cada episodio del loop principal
- Limpia memoria y reintenta el episodio completo
- Guarda checkpoint antes de detener si error persistente
- Limpieza peri√≥dica cada 20 episodios o despu√©s de guardar checkpoint

**C√≥digo:**
```python
for episode in range(start_episode, total_episodes):
    try:
        loss, metrics = trainer.train_episode(episode)
        # ... logging y guardado ...
    except torch.cuda.OutOfMemoryError as e:
        logging.error(f"‚ùå CUDA Out of Memory en episodio {episode}: {e}")
        # Limpiar y reintentar
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
            try:
                loss, metrics = trainer.train_episode(episode)
                logging.info(f"‚úÖ Episodio {episode} completado despu√©s de limpiar memoria")
            except torch.cuda.OutOfMemoryError:
                # Guardar checkpoint y detener
                trainer.save_checkpoint(episode - 1 if episode > 0 else 0, ...)
                raise
    
    # Limpiar cach√© peri√≥dicamente
    if (episode + 1) % 20 == 0 or (episode + 1) % save_every == 0:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            gc.collect()
```

### Estrategias de Limpieza de Memoria

1. **Limpieza Peri√≥dica:**
   - Cada 10 episodios en `train_episode()` (despu√©s de calcular p√©rdida)
   - Cada 20 episodios en loop principal
   - Despu√©s de guardar cada checkpoint

2. **Limpieza Reactiva:**
   - Cuando ocurre OutOfMemoryError (antes de reintentar)
   - Despu√©s de eliminar `psi_history` (ya exist√≠a)

3. **Recuperaci√≥n Autom√°tica:**
   - Reintento inmediato despu√©s de limpiar memoria
   - Si persiste, guarda checkpoint y detiene gracefulmente

### Beneficios

- ‚úÖ **Reducci√≥n de errores:** Limpieza peri√≥dica previene acumulaci√≥n de memoria
- ‚úÖ **Recuperaci√≥n autom√°tica:** Reintento despu√©s de limpiar memoria
- ‚úÖ **Preservaci√≥n de progreso:** Guarda checkpoint antes de detener si error persistente
- ‚úÖ **Mejor estabilidad:** Menos interrupciones durante entrenamientos largos

### Consideraciones

- La limpieza peri√≥dica a√±ade un peque√±o overhead (~1-2ms por episodio)
- El reintento puede duplicar el tiempo de un episodio si ocurre error
- Si el error persiste despu√©s del reintento, indica que el modelo es demasiado grande para la GPU disponible

### Soluciones Alternativas si Persiste

Si el error persiste frecuentemente:
1. **Reducir tama√±o del modelo:** `hid_dim`, `num_layers`, etc.
2. **Reducir tama√±o del grid:** `GRID_SIZE_TRAINING` (ej: 64 ‚Üí 32)
3. **Reducir pasos QCA:** `QCA_STEPS_TRAINING` (ej: 100 ‚Üí 50)
4. **Usar mixed precision:** `torch.cuda.amp` (entrenamiento con FP16)
5. **Gradient checkpointing:** Ya comentado en c√≥digo, se puede activar

### Referencias
- [[NATIVE_ENGINE_PERFORMANCE_ISSUES]]
- [[CHECKPOINT_STATE_ANALYSIS]]
- [PyTorch Memory Management](https://pytorch.org/docs/stable/notes/cuda.html#memory-management)

---



---
[[AI_DEV_LOG|üîô Volver al √çndice]]

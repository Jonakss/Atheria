# An√°lisis de T√©cnicas Avanzadas para QCA Unitaria

## 1. RMSNorm (Root Mean Square Normalization)

### ¬øQu√© es?
RMSNorm es una normalizaci√≥n que **NO resta la media**. Solo divide por la ra√≠z cuadrada de la media de los cuadrados:

```
RMSNorm(x) = x / sqrt(mean(x¬≤) + Œµ)
```

vs GroupNorm (que usas ahora):
```
GroupNorm(x) = (x - mean(x)) / sqrt(var(x) + Œµ)
```

### ¬øPor qu√© es importante para QCA Unitaria?

**‚úÖ BENEFICIO CR√çTICO**: Tu f√≠sica cu√°ntica preserva la **energ√≠a total** del sistema (`|œà|¬≤`). GroupNorm fuerza `mean(x) = 0`, lo que puede distorsionar esta conservaci√≥n de energ√≠a.

**Ejemplo del problema actual:**
```python
# Estado cu√°ntico con energ√≠a concentrada
psi = [0.1, 0.1, 0.8, 0.1]  # Energ√≠a total = 0.67

# Despu√©s de GroupNorm (fuerza media=0):
psi_norm = GroupNorm(psi)  # Media ‚âà 0, pero la energ√≠a se distorsiona

# Despu√©s de RMSNorm (preserva escala):
psi_norm = RMSNorm(psi)  # Mantiene la proporci√≥n de energ√≠a
```

### Costo Computacional

**Velocidad**: ‚ö° **M√ÅS R√ÅPIDO** que GroupNorm
- GroupNorm: Calcula `mean` y `var` (2 operaciones)
- RMSNorm: Solo calcula `mean(x¬≤)` (1 operaci√≥n)
- **Ahorro**: ~15-20% m√°s r√°pido en normalizaci√≥n

**Memoria**: Igual que GroupNorm (mismo overhead)

### Implementaci√≥n Recomendada

```python
class RMSNorm2d(nn.Module):
    """RMSNorm para tensores 2D (im√°genes)"""
    def __init__(self, num_channels, eps=1e-6):
        super().__init__()
        self.num_channels = num_channels
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(num_channels))
    
    def forward(self, x):
        # x: [B, C, H, W]
        # Calcular RMS por canal
        rms = torch.sqrt(torch.mean(x**2, dim=[2, 3], keepdim=True) + self.eps)
        # Normalizar y escalar
        x_norm = x / rms * self.weight.view(1, -1, 1, 1)
        return x_norm
```

### Recomendaci√≥n: ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê **IMPLEMENTAR PRIMERO**

**Razones:**
1. M√°s r√°pido que GroupNorm
2. Preserva mejor la energ√≠a (cr√≠tico para f√≠sica unitaria)
3. Implementaci√≥n simple (solo reemplazar GroupNorm)
4. Mejora estabilidad del entrenamiento

**Impacto esperado:**
- **Velocidad**: +15-20% m√°s r√°pido
- **Estabilidad**: Mejor conservaci√≥n de energ√≠a
- **Calidad**: Posible mejora en patrones emergentes

---

## 2. SwiGLU (Swish Gated Linear Unit)

### ¬øQu√© es?
SwiGLU es una activaci√≥n "gated" (con puerta):

```
SwiGLU(x) = Swish(xW‚ÇÅ + b‚ÇÅ) ‚äô (xW‚ÇÇ + b‚ÇÇ)
```

donde `Swish(x) = x * sigmoid(x)` y `‚äô` es multiplicaci√≥n elemento a elemento.

### ¬øPor qu√© podr√≠a ayudar?

**‚úÖ BENEFICIO**: Permite que la red aprenda "condicionales" complejos:
- "Si hay mucha energ√≠a aqu√≠, deja pasar el flujo"
- "Si el vecindario est√° vac√≠o, bloquea la propagaci√≥n"

**‚ùå PROBLEMA**: Tu U-Net ya tiene skip connections que hacen algo similar. SwiGLU a√±ade complejidad sin garant√≠a de mejora.

### Costo Computacional

**Velocidad**: üêå **M√ÅS LENTO** que ELU
- ELU: 1 operaci√≥n (`elu(x)`)
- SwiGLU: 2 convoluciones + 1 sigmoid + 1 multiplicaci√≥n
- **Costo**: ~2-3√ó m√°s lento que ELU

**Memoria**: ~2√ó m√°s (necesita almacenar 2 transformaciones lineales)

### Implementaci√≥n

```python
class SwiGLU(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.gate_proj = nn.Linear(dim, dim)
        self.up_proj = nn.Linear(dim, dim)
    
    def forward(self, x):
        # Para Conv2d, necesitar√≠as adaptar esto
        gate = torch.sigmoid(self.gate_proj(x))
        up = self.up_proj(x)
        return gate * up
```

### Recomendaci√≥n: ‚≠ê‚≠ê **NO IMPLEMENTAR (por ahora)**

**Razones:**
1. **Costo alto** (2-3√ó m√°s lento)
2. **Beneficio incierto** (tu U-Net ya tiene skip connections)
3. **Complejidad** (necesita adaptaci√≥n para Conv2d)
4. **Prioridad baja** (hay mejoras m√°s impactantes primero)

**Cu√°ndo considerar:**
- Si despu√©s de implementar RMSNorm y RoPE sigues teniendo problemas de aprendizaje
- Si quieres experimentar con arquitecturas m√°s complejas

---

## 3. RoPE (Rotary Positional Embeddings)

### ¬øQu√© es?
RoPE codifica la posici√≥n mediante **rotaciones en el plano complejo**:

```
RoPE(x, pos) = x * e^(i * Œ∏ * pos)
```

donde `Œ∏` es una frecuencia aprendida y `pos` es la posici√≥n (x, y en tu caso).

### ¬øPor qu√© es FUNDAMENTAL para QCA?

**‚úÖ BENEFICIO CR√çTICO**: Tu f√≠sica unitaria **YA ES UNA ROTACI√ìN**:

```python
# Tu evoluci√≥n unitaria:
psi(t+1) = U * psi(t)  # U es una matriz unitaria = rotaci√≥n

# RoPE expl√≠citamente codifica rotaciones:
psi_rope = psi * exp(i * Œ∏ * (x, y))  # Rotaci√≥n expl√≠cita por posici√≥n
```

**Esto significa:**
- RoPE puede ayudar a tu U-Net a **entender la geometr√≠a del espacio** de forma natural
- Las convoluciones 3√ó3 ven "vecindarios", pero RoPE ve "direcciones" y "distancias angulares"
- **Perfecto para patrones rotacionales** (gliders, v√≥rtices, ondas)

### Costo Computacional

**Velocidad**: ‚ö°‚ö° **LIGERAMENTE M√ÅS LENTO** que sin RoPE
- RoPE a√±ade: c√°lculo de rotaciones complejas por posici√≥n
- **Costo**: ~10-15% m√°s lento
- **Pero**: Puede permitir reducir el n√∫mero de capas (mejor eficiencia global)

**Memoria**: +O(H*W) para almacenar frecuencias posicionales (m√≠nimo)

### Implementaci√≥n Recomendada

```python
class RoPE2d(nn.Module):
    """RoPE para im√°genes 2D"""
    def __init__(self, dim, max_freq=10000.0):
        super().__init__()
        self.dim = dim
        # Frecuencias para cada dimensi√≥n (x, y)
        inv_freq = 1.0 / (max_freq ** (torch.arange(0, dim, 2).float() / dim))
        self.register_buffer('inv_freq', inv_freq)
    
    def forward(self, x):
        # x: [B, C, H, W]
        B, C, H, W = x.shape
        
        # Crear grid de posiciones
        y_pos = torch.arange(H, device=x.device).float()
        x_pos = torch.arange(W, device=x.device).float()
        
        # Calcular √°ngulos de rotaci√≥n
        theta_y = torch.outer(y_pos, self.inv_freq)  # [H, dim//2]
        theta_x = torch.outer(x_pos, self.inv_freq)  # [W, dim//2]
        
        # Aplicar rotaci√≥n (simplificado - necesitar√≠a adaptaci√≥n para Conv2d)
        # En la pr√°ctica, esto se aplicar√≠a a los embeddings de posici√≥n
        # antes de las convoluciones
        
        return x  # Placeholder
```

### Recomendaci√≥n: ‚≠ê‚≠ê‚≠ê‚≠ê **IMPLEMENTAR DESPU√âS DE RMSNorm**

**Razones:**
1. **Alto impacto potencial** para f√≠sica rotacional
2. **Costo moderado** (~10-15% m√°s lento)
3. **Complejidad media** (necesita dise√±o cuidadoso para Conv2d)
4. **Sinergia con f√≠sica unitaria** (rotaciones expl√≠citas)

**Impacto esperado:**
- **Calidad**: Mejor aprendizaje de patrones rotacionales
- **Estabilidad**: Mejor comprensi√≥n de geometr√≠a espacial
- **Emergencia**: Posible mejora en gliders y estructuras complejas

---

## Plan de Implementaci√≥n Recomendado

### Fase 1: RMSNorm (Prioridad Alta) ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
- **Tiempo**: 1-2 horas
- **Beneficio**: +15-20% velocidad, mejor conservaci√≥n de energ√≠a
- **Riesgo**: Bajo (reemplazo directo)

### Fase 2: RoPE (Prioridad Media) ‚≠ê‚≠ê‚≠ê‚≠ê
- **Tiempo**: 4-6 horas (dise√±o cuidadoso)
- **Beneficio**: Mejor geometr√≠a espacial, patrones rotacionales
- **Riesgo**: Medio (necesita experimentaci√≥n)

### Fase 3: SwiGLU (Prioridad Baja) ‚≠ê‚≠ê
- **Tiempo**: 2-3 horas
- **Beneficio**: Incierto
- **Riesgo**: Alto (costo computacional)

---

## Resumen de Costos/Beneficios

| T√©cnica | Velocidad | Memoria | Beneficio QCA | Dificultad | Prioridad |
|---------|-----------|---------|---------------|------------|-----------|
| **RMSNorm** | ‚ö°‚ö°‚ö° (+20%) | = | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | **ALTA** |
| **RoPE** | ‚ö°‚ö° (-15%) | + | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | **MEDIA** |
| **SwiGLU** | üêå (-200%) | ++ | ‚≠ê‚≠ê | ‚≠ê‚≠ê | **BAJA** |

---

## Conclusi√≥n

**Implementa primero RMSNorm** - Es r√°pido, simple, y mejora la conservaci√≥n de energ√≠a (cr√≠tico para tu f√≠sica).

**Luego considera RoPE** - Si quieres mejorar patrones rotacionales y geometr√≠a espacial.

**Evita SwiGLU por ahora** - El costo no justifica el beneficio incierto.

